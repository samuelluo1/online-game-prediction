{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19941\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('KRfin01.csv')\n",
    "df2 = pd.read_csv('KRfin02.csv')\n",
    "df = df.append(df2, ignore_index=True)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,val_X,test_X=np.array(df.iloc[:15952,1:-1]),np.array(df.iloc[15952::2,1:-1]),np.array(df.iloc[15953::2,1:-1])\n",
    "train_Y,val_Y,test_Y=np.array(df.iloc[:15952,-1]),np.array(df.iloc[15952::2,-1]),np.array(df.iloc[15953::2,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train set(15952): 7928 win and 8024 lose\n",
      "In val set(1995): 960 win and 1035 lose\n",
      "In test set(1994): 975 win and 1019 lose\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(f\"In train set(15952): {Counter(train_Y)[0]} win and {Counter(train_Y)[1]} lose\")\n",
    "print(f\"In val set(1995): {Counter(val_Y)[0]} win and {Counter(val_Y)[1]} lose\")\n",
    "print(f\"In test set(1994): {Counter(test_Y)[0]} win and {Counter(test_Y)[1]} lose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TensorDataset(torch.from_numpy(train_X),torch.from_numpy(train_Y))\n",
    "val_data=TensorDataset(torch.from_numpy(val_X),torch.from_numpy(val_Y))\n",
    "test_data=TensorDataset(torch.from_numpy(test_X),torch.from_numpy(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the number positive and negative examples in the training set\n",
    "num_classes=[Counter(train_Y)[0],Counter(train_Y)[1]]\n",
    "#find the respective weights (mind that weights need to be a torch tensor)\n",
    "weights=1./torch.tensor(num_classes,dtype=float)\n",
    "#create a torch tensor associating the train_Y and the weights. Sample Weights is a torch tensor\n",
    "sample_weights=weights[train_Y]\n",
    "sampler=torch.utils.data.WeightedRandomSampler(weights=sample_weights,num_samples=len(sample_weights),replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "train_loader=DataLoader(train_data,sampler=sampler,batch_size=batch_size)\n",
    "val_loader=DataLoader(val_data,shuffle=True,batch_size=batch_size)\n",
    "test_loader=DataLoader(test_data,shuffle=False,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4426, 0.4155, 0.4834,  ..., 0.5186, 0.4681, 0.5037],\n",
      "        [0.5098, 0.4632, 0.5251,  ..., 0.4392, 0.4158, 0.4761],\n",
      "        [0.4891, 0.5416, 0.5335,  ..., 0.4737, 0.4982, 0.4512],\n",
      "        ...,\n",
      "        [0.5235, 0.4011, 0.4558,  ..., 0.4432, 0.5630, 0.5413],\n",
      "        [0.4623, 0.5613, 0.4032,  ..., 0.4693, 0.5359, 0.4959],\n",
      "        [0.4462, 0.5312, 0.3702,  ..., 0.4293, 0.5428, 0.5563]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "train_iter=iter(val_loader)\n",
    "inputs,outputs=train_iter.next()\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 23])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader wrapping\n",
    "loaders={'train':train_loader, 'valid':val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(23,32)\n",
    "        self.norm1=nn.BatchNorm1d(32)\n",
    "        self.fc2=nn.Linear(32,16,bias=False)\n",
    "        self.norm2=nn.BatchNorm1d(16)\n",
    "        self.fc3=nn.Linear(16,8,bias=False)\n",
    "        self.norm3=nn.BatchNorm1d(8)\n",
    "        self.fc4=nn.Linear(8,5,bias=False)\n",
    "        self.norm4=nn.BatchNorm1d(5)\n",
    "        self.fc5=nn.Linear(5,1)\n",
    "         \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.norm1(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x=F.relu(self.norm2(self.fc2(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x=F.relu(self.norm3(self.fc3(x)))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x=F.relu(self.norm4(self.fc4(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x=self.fc5(x)        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=23, out_features=32, bias=True)\n",
       "  (norm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=16, out_features=8, bias=False)\n",
       "  (norm3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=8, out_features=5, bias=False)\n",
       "  (norm4): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc5): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-4)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",factor=0.75,patience=50,min_lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs,loaders,model,optimizer,criterion,scheduler):\n",
    "    valid_loss_min=np.Inf\n",
    "    list_train_loss=[]\n",
    "    list_valid_loss=[]\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "        model.train()\n",
    "        for batch_idx,(data,target) in enumerate(loaders[\"train\"]):\n",
    "            optimizer.zero_grad()\n",
    "            output=model(data.float())\n",
    "            loss=criterion(output.squeeze(),target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss=train_loss+1/(batch_idx+1)*(loss.item()-train_loss)\n",
    "        list_train_loss.append(train_loss)\n",
    "        print(\"At {} epoch, Training Loss: {} \".format(epoch,train_loss))\n",
    "        model.eval()\n",
    "        for batch_idx,(data,target) in enumerate(loaders[\"valid\"]):\n",
    "            output=model(data.float())\n",
    "            loss=criterion(output.squeeze(),target.float())\n",
    "            valid_loss=valid_loss+1/(batch_idx+1)*(loss.item()-valid_loss)\n",
    "        scheduler.step(valid_loss)\n",
    "        list_valid_loss.append(valid_loss)\n",
    "        print(\"At {} epoch, Validation Loss: {} \".format(epoch,valid_loss))\n",
    "        #Save the model\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(),'ccFraud.pt')\n",
    "            print(\"Minimum validation loss detected, saving model......................................................................................\")\n",
    "            valid_loss_min=valid_loss\n",
    "    \n",
    "    return model, list_train_loss, list_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 1 epoch, Training Loss: 0.7133112132549282 \n",
      "At 1 epoch, Validation Loss: 0.695347875356674 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 2 epoch, Training Loss: 0.6999443348497147 \n",
      "At 2 epoch, Validation Loss: 0.6949033975601195 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 3 epoch, Training Loss: 0.6962712775915862 \n",
      "At 3 epoch, Validation Loss: 0.694761747121811 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 4 epoch, Training Loss: 0.69490125477314 \n",
      "At 4 epoch, Validation Loss: 0.694350266456604 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 5 epoch, Training Loss: 0.6942965764552355 \n",
      "At 5 epoch, Validation Loss: 0.694116958975792 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 6 epoch, Training Loss: 0.6934666886925696 \n",
      "At 6 epoch, Validation Loss: 0.6937220275402067 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 7 epoch, Training Loss: 0.6935343086719509 \n",
      "At 7 epoch, Validation Loss: 0.6938329428434371 \n",
      "At 8 epoch, Training Loss: 0.6931667927652599 \n",
      "At 8 epoch, Validation Loss: 0.6933981537818908 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 9 epoch, Training Loss: 0.6932495806366207 \n",
      "At 9 epoch, Validation Loss: 0.6938509941101074 \n",
      "At 10 epoch, Training Loss: 0.6932891540229322 \n",
      "At 10 epoch, Validation Loss: 0.693669781088829 \n",
      "At 11 epoch, Training Loss: 0.6933630142360921 \n",
      "At 11 epoch, Validation Loss: 0.6936547219753263 \n",
      "At 12 epoch, Training Loss: 0.693122112378478 \n",
      "At 12 epoch, Validation Loss: 0.693227618932724 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 13 epoch, Training Loss: 0.6928892247378826 \n",
      "At 13 epoch, Validation Loss: 0.6931117206811905 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 14 epoch, Training Loss: 0.6929172638803719 \n",
      "At 14 epoch, Validation Loss: 0.6935680627822877 \n",
      "At 15 epoch, Training Loss: 0.6930486962199212 \n",
      "At 15 epoch, Validation Loss: 0.6935535252094268 \n",
      "At 16 epoch, Training Loss: 0.6932148832827806 \n",
      "At 16 epoch, Validation Loss: 0.6933712482452392 \n",
      "At 17 epoch, Training Loss: 0.6928426243364812 \n",
      "At 17 epoch, Validation Loss: 0.6933396100997926 \n",
      "At 18 epoch, Training Loss: 0.6926516700536014 \n",
      "At 18 epoch, Validation Loss: 0.6933643370866777 \n",
      "At 19 epoch, Training Loss: 0.6925764363259077 \n",
      "At 19 epoch, Validation Loss: 0.6932914674282074 \n",
      "At 20 epoch, Training Loss: 0.6929247576743361 \n",
      "At 20 epoch, Validation Loss: 0.6931142419576645 \n",
      "At 21 epoch, Training Loss: 0.6923998720943934 \n",
      "At 21 epoch, Validation Loss: 0.6929090529680252 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 22 epoch, Training Loss: 0.69213838391006 \n",
      "At 22 epoch, Validation Loss: 0.6931851953268052 \n",
      "At 23 epoch, Training Loss: 0.6928134597837927 \n",
      "At 23 epoch, Validation Loss: 0.6930534541606902 \n",
      "At 24 epoch, Training Loss: 0.6926104329526422 \n",
      "At 24 epoch, Validation Loss: 0.6927150368690489 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 25 epoch, Training Loss: 0.6926910143345593 \n",
      "At 25 epoch, Validation Loss: 0.6927461832761764 \n",
      "At 26 epoch, Training Loss: 0.6923836443573241 \n",
      "At 26 epoch, Validation Loss: 0.6924437940120698 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 27 epoch, Training Loss: 0.6923623081296679 \n",
      "At 27 epoch, Validation Loss: 0.6924761205911637 \n",
      "At 28 epoch, Training Loss: 0.692567639425397 \n",
      "At 28 epoch, Validation Loss: 0.6925681918859482 \n",
      "At 29 epoch, Training Loss: 0.6917502734810118 \n",
      "At 29 epoch, Validation Loss: 0.6919110357761384 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 30 epoch, Training Loss: 0.6923740565776826 \n",
      "At 30 epoch, Validation Loss: 0.6921147614717483 \n",
      "At 31 epoch, Training Loss: 0.6923124164342881 \n",
      "At 31 epoch, Validation Loss: 0.6923028349876404 \n",
      "At 32 epoch, Training Loss: 0.6919735353440045 \n",
      "At 32 epoch, Validation Loss: 0.6924877285957338 \n",
      "At 33 epoch, Training Loss: 0.6921411592513322 \n",
      "At 33 epoch, Validation Loss: 0.6918548166751861 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 34 epoch, Training Loss: 0.691282356157899 \n",
      "At 34 epoch, Validation Loss: 0.6923010915517807 \n",
      "At 35 epoch, Training Loss: 0.6910933803766962 \n",
      "At 35 epoch, Validation Loss: 0.691533187031746 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 36 epoch, Training Loss: 0.691438452154398 \n",
      "At 36 epoch, Validation Loss: 0.6915277153253557 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 37 epoch, Training Loss: 0.6909565743058924 \n",
      "At 37 epoch, Validation Loss: 0.6924934327602388 \n",
      "At 38 epoch, Training Loss: 0.6905988387763501 \n",
      "At 38 epoch, Validation Loss: 0.6929259210824966 \n",
      "At 39 epoch, Training Loss: 0.690230222418904 \n",
      "At 39 epoch, Validation Loss: 0.6910389751195907 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 40 epoch, Training Loss: 0.6915946871042251 \n",
      "At 40 epoch, Validation Loss: 0.6916941910982133 \n",
      "At 41 epoch, Training Loss: 0.6913593575358391 \n",
      "At 41 epoch, Validation Loss: 0.6918677389621735 \n",
      "At 42 epoch, Training Loss: 0.6912805043160914 \n",
      "At 42 epoch, Validation Loss: 0.6912160903215409 \n",
      "At 43 epoch, Training Loss: 0.6907388981431722 \n",
      "At 43 epoch, Validation Loss: 0.6924471259117125 \n",
      "At 44 epoch, Training Loss: 0.6917391944676635 \n",
      "At 44 epoch, Validation Loss: 0.6914493709802629 \n",
      "At 45 epoch, Training Loss: 0.6910489421337843 \n",
      "At 45 epoch, Validation Loss: 0.6925593793392181 \n",
      "At 46 epoch, Training Loss: 0.6891201432794332 \n",
      "At 46 epoch, Validation Loss: 0.6911323726177215 \n",
      "At 47 epoch, Training Loss: 0.6908073719590905 \n",
      "At 47 epoch, Validation Loss: 0.6908762991428375 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 48 epoch, Training Loss: 0.6902788586914533 \n",
      "At 48 epoch, Validation Loss: 0.6916209161281587 \n",
      "At 49 epoch, Training Loss: 0.6898443508893254 \n",
      "At 49 epoch, Validation Loss: 0.6923931926488875 \n",
      "At 50 epoch, Training Loss: 0.6905466612428426 \n",
      "At 50 epoch, Validation Loss: 0.6915456801652908 \n",
      "At 51 epoch, Training Loss: 0.6903149887919428 \n",
      "At 51 epoch, Validation Loss: 0.6910636454820632 \n",
      "At 52 epoch, Training Loss: 0.6896096397191288 \n",
      "At 52 epoch, Validation Loss: 0.6911595463752747 \n",
      "At 53 epoch, Training Loss: 0.6905806038528686 \n",
      "At 53 epoch, Validation Loss: 0.6911841362714768 \n",
      "At 54 epoch, Training Loss: 0.6887575175613162 \n",
      "At 54 epoch, Validation Loss: 0.6916092514991761 \n",
      "At 55 epoch, Training Loss: 0.6910932175815104 \n",
      "At 55 epoch, Validation Loss: 0.6907470732927321 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 56 epoch, Training Loss: 0.6908095654100181 \n",
      "At 56 epoch, Validation Loss: 0.6916214466094969 \n",
      "At 57 epoch, Training Loss: 0.6905161235481498 \n",
      "At 57 epoch, Validation Loss: 0.6904712349176406 \n",
      "Minimum validation loss detected, saving model......................................................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 58 epoch, Training Loss: 0.6909635182470083 \n",
      "At 58 epoch, Validation Loss: 0.6904861241579056 \n",
      "At 59 epoch, Training Loss: 0.6886950567364695 \n",
      "At 59 epoch, Validation Loss: 0.6910192131996155 \n",
      "At 60 epoch, Training Loss: 0.68890317119658 \n",
      "At 60 epoch, Validation Loss: 0.6910494476556777 \n",
      "At 61 epoch, Training Loss: 0.6900945920497179 \n",
      "At 61 epoch, Validation Loss: 0.6911063939332962 \n",
      "At 62 epoch, Training Loss: 0.6900424983352427 \n",
      "At 62 epoch, Validation Loss: 0.6913325399160384 \n",
      "At 63 epoch, Training Loss: 0.6898618180304763 \n",
      "At 63 epoch, Validation Loss: 0.6919868677854538 \n",
      "At 64 epoch, Training Loss: 0.6893341124057768 \n",
      "At 64 epoch, Validation Loss: 0.6915670275688172 \n",
      "At 65 epoch, Training Loss: 0.6903624597936864 \n",
      "At 65 epoch, Validation Loss: 0.6918910443782805 \n",
      "At 66 epoch, Training Loss: 0.6876327294856309 \n",
      "At 66 epoch, Validation Loss: 0.693524518609047 \n",
      "At 67 epoch, Training Loss: 0.6902521904557946 \n",
      "At 67 epoch, Validation Loss: 0.6915023148059845 \n",
      "At 68 epoch, Training Loss: 0.6899019364267583 \n",
      "At 68 epoch, Validation Loss: 0.6913196593523024 \n",
      "At 69 epoch, Training Loss: 0.6887966629117727 \n",
      "At 69 epoch, Validation Loss: 0.6914053320884705 \n",
      "At 70 epoch, Training Loss: 0.6894060868769885 \n",
      "At 70 epoch, Validation Loss: 0.6910287350416183 \n",
      "At 71 epoch, Training Loss: 0.6899132337421179 \n",
      "At 71 epoch, Validation Loss: 0.6916144788265228 \n",
      "At 72 epoch, Training Loss: 0.6894645456224683 \n",
      "At 72 epoch, Validation Loss: 0.6904349476099013 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 73 epoch, Training Loss: 0.6900040071457624 \n",
      "At 73 epoch, Validation Loss: 0.6910802334547044 \n",
      "At 74 epoch, Training Loss: 0.6898098185658454 \n",
      "At 74 epoch, Validation Loss: 0.6906339854001998 \n",
      "At 75 epoch, Training Loss: 0.6897125713527201 \n",
      "At 75 epoch, Validation Loss: 0.6914217144250869 \n",
      "At 76 epoch, Training Loss: 0.6894953593611713 \n",
      "At 76 epoch, Validation Loss: 0.6915896773338318 \n",
      "At 77 epoch, Training Loss: 0.6897484283894303 \n",
      "At 77 epoch, Validation Loss: 0.691078606247902 \n",
      "At 78 epoch, Training Loss: 0.6893592778593298 \n",
      "At 78 epoch, Validation Loss: 0.6909129679203033 \n",
      "At 79 epoch, Training Loss: 0.6896491315215826 \n",
      "At 79 epoch, Validation Loss: 0.6911582082509995 \n",
      "At 80 epoch, Training Loss: 0.689193129166961 \n",
      "At 80 epoch, Validation Loss: 0.6913496255874633 \n",
      "At 81 epoch, Training Loss: 0.6888950027525425 \n",
      "At 81 epoch, Validation Loss: 0.6912204295396804 \n",
      "At 82 epoch, Training Loss: 0.6883931424468751 \n",
      "At 82 epoch, Validation Loss: 0.6927358061075211 \n",
      "At 83 epoch, Training Loss: 0.6875812869518995 \n",
      "At 83 epoch, Validation Loss: 0.6914961636066436 \n",
      "At 84 epoch, Training Loss: 0.6886985477060078 \n",
      "At 84 epoch, Validation Loss: 0.6909729719161988 \n",
      "At 85 epoch, Training Loss: 0.6889225676655771 \n",
      "At 85 epoch, Validation Loss: 0.6902884602546692 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 86 epoch, Training Loss: 0.6895050443708896 \n",
      "At 86 epoch, Validation Loss: 0.6912708044052124 \n",
      "At 87 epoch, Training Loss: 0.6884757328778505 \n",
      "At 87 epoch, Validation Loss: 0.690396198630333 \n",
      "At 88 epoch, Training Loss: 0.6885395612567663 \n",
      "At 88 epoch, Validation Loss: 0.69109405875206 \n",
      "At 89 epoch, Training Loss: 0.6874905835837123 \n",
      "At 89 epoch, Validation Loss: 0.6928759247064591 \n",
      "At 90 epoch, Training Loss: 0.6891851313412192 \n",
      "At 90 epoch, Validation Loss: 0.6913256853818893 \n",
      "At 91 epoch, Training Loss: 0.6896725941449402 \n",
      "At 91 epoch, Validation Loss: 0.691510796546936 \n",
      "At 92 epoch, Training Loss: 0.6887960739433764 \n",
      "At 92 epoch, Validation Loss: 0.6918318808078767 \n",
      "At 93 epoch, Training Loss: 0.689671256765723 \n",
      "At 93 epoch, Validation Loss: 0.6924895375967025 \n",
      "At 94 epoch, Training Loss: 0.6890613298863173 \n",
      "At 94 epoch, Validation Loss: 0.6917023688554765 \n",
      "At 95 epoch, Training Loss: 0.6881038010120393 \n",
      "At 95 epoch, Validation Loss: 0.6909455776214599 \n",
      "At 96 epoch, Training Loss: 0.6889887992292641 \n",
      "At 96 epoch, Validation Loss: 0.6941669523715973 \n",
      "At 97 epoch, Training Loss: 0.6892164409160609 \n",
      "At 97 epoch, Validation Loss: 0.6918050110340118 \n",
      "At 98 epoch, Training Loss: 0.6886396810412407 \n",
      "At 98 epoch, Validation Loss: 0.6926118195056914 \n",
      "At 99 epoch, Training Loss: 0.688082157075405 \n",
      "At 99 epoch, Validation Loss: 0.6915164291858673 \n",
      "At 100 epoch, Training Loss: 0.6882018938660617 \n",
      "At 100 epoch, Validation Loss: 0.6917982637882233 \n",
      "At 101 epoch, Training Loss: 0.6891529325395823 \n",
      "At 101 epoch, Validation Loss: 0.6923169016838073 \n",
      "At 102 epoch, Training Loss: 0.6895751968026161 \n",
      "At 102 epoch, Validation Loss: 0.6918780505657195 \n",
      "At 103 epoch, Training Loss: 0.6884347699582578 \n",
      "At 103 epoch, Validation Loss: 0.6916355580091477 \n",
      "At 104 epoch, Training Loss: 0.6882273383438587 \n",
      "At 104 epoch, Validation Loss: 0.6904297351837159 \n",
      "At 105 epoch, Training Loss: 0.6876762080937621 \n",
      "At 105 epoch, Validation Loss: 0.6906233370304109 \n",
      "At 106 epoch, Training Loss: 0.6879695933312175 \n",
      "At 106 epoch, Validation Loss: 0.6909861356019974 \n",
      "At 107 epoch, Training Loss: 0.6888582710176704 \n",
      "At 107 epoch, Validation Loss: 0.6909311026334762 \n",
      "At 108 epoch, Training Loss: 0.6884723018854858 \n",
      "At 108 epoch, Validation Loss: 0.6909027814865112 \n",
      "At 109 epoch, Training Loss: 0.6868049528449771 \n",
      "At 109 epoch, Validation Loss: 0.6915338158607484 \n",
      "At 110 epoch, Training Loss: 0.6890422813594336 \n",
      "At 110 epoch, Validation Loss: 0.6916248023509979 \n",
      "At 111 epoch, Training Loss: 0.6893309634178875 \n",
      "At 111 epoch, Validation Loss: 0.691081064939499 \n",
      "At 112 epoch, Training Loss: 0.6877741817384959 \n",
      "At 112 epoch, Validation Loss: 0.6913285195827484 \n",
      "At 113 epoch, Training Loss: 0.6862935405224563 \n",
      "At 113 epoch, Validation Loss: 0.6902973920106888 \n",
      "At 114 epoch, Training Loss: 0.6878067024052147 \n",
      "At 114 epoch, Validation Loss: 0.6916784107685089 \n",
      "At 115 epoch, Training Loss: 0.6878183141350747 \n",
      "At 115 epoch, Validation Loss: 0.6913368493318558 \n",
      "At 116 epoch, Training Loss: 0.6892441041767596 \n",
      "At 116 epoch, Validation Loss: 0.6909739792346954 \n",
      "At 117 epoch, Training Loss: 0.6876874584704635 \n",
      "At 117 epoch, Validation Loss: 0.6900901377201081 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 118 epoch, Training Loss: 0.6883662626147271 \n",
      "At 118 epoch, Validation Loss: 0.6902658492326736 \n",
      "At 119 epoch, Training Loss: 0.6892125409096479 \n",
      "At 119 epoch, Validation Loss: 0.6901015847921371 \n",
      "At 120 epoch, Training Loss: 0.6877747133374214 \n",
      "At 120 epoch, Validation Loss: 0.6914636492729187 \n",
      "At 121 epoch, Training Loss: 0.6871454600244762 \n",
      "At 121 epoch, Validation Loss: 0.691254910826683 \n",
      "At 122 epoch, Training Loss: 0.6890301674604418 \n",
      "At 122 epoch, Validation Loss: 0.6907679259777068 \n",
      "At 123 epoch, Training Loss: 0.687318076565862 \n",
      "At 123 epoch, Validation Loss: 0.6907652437686921 \n",
      "At 124 epoch, Training Loss: 0.6868690330535173 \n",
      "At 124 epoch, Validation Loss: 0.6904272735118867 \n",
      "At 125 epoch, Training Loss: 0.6888685029000041 \n",
      "At 125 epoch, Validation Loss: 0.6909325361251831 \n",
      "At 126 epoch, Training Loss: 0.6884022373706107 \n",
      "At 126 epoch, Validation Loss: 0.6922699570655824 \n",
      "At 127 epoch, Training Loss: 0.6878192279487846 \n",
      "At 127 epoch, Validation Loss: 0.6905809640884399 \n",
      "At 128 epoch, Training Loss: 0.6885733313858508 \n",
      "At 128 epoch, Validation Loss: 0.6907791256904602 \n",
      "At 129 epoch, Training Loss: 0.68948136754334 \n",
      "At 129 epoch, Validation Loss: 0.691504141688347 \n",
      "At 130 epoch, Training Loss: 0.6881649609655143 \n",
      "At 130 epoch, Validation Loss: 0.6908607333898544 \n",
      "At 131 epoch, Training Loss: 0.6866222511976957 \n",
      "At 131 epoch, Validation Loss: 0.6911839246749878 \n",
      "At 132 epoch, Training Loss: 0.6865121662616731 \n",
      "At 132 epoch, Validation Loss: 0.691230136156082 \n",
      "At 133 epoch, Training Loss: 0.6869588900357484 \n",
      "At 133 epoch, Validation Loss: 0.6901822537183762 \n",
      "At 134 epoch, Training Loss: 0.6868274956941606 \n",
      "At 134 epoch, Validation Loss: 0.6905653715133667 \n",
      "At 135 epoch, Training Loss: 0.6881426412612199 \n",
      "At 135 epoch, Validation Loss: 0.6921372175216675 \n",
      "At 136 epoch, Training Loss: 0.6878691021353004 \n",
      "At 136 epoch, Validation Loss: 0.6911687791347503 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 137 epoch, Training Loss: 0.6867939386516813 \n",
      "At 137 epoch, Validation Loss: 0.6914768487215043 \n",
      "At 138 epoch, Training Loss: 0.6860365461558111 \n",
      "At 138 epoch, Validation Loss: 0.6919278174638748 \n",
      "At 139 epoch, Training Loss: 0.6872687600553036 \n",
      "At 139 epoch, Validation Loss: 0.6912701606750488 \n",
      "At 140 epoch, Training Loss: 0.686841654777527 \n",
      "At 140 epoch, Validation Loss: 0.6921200096607207 \n",
      "At 141 epoch, Training Loss: 0.6863680694252255 \n",
      "At 141 epoch, Validation Loss: 0.6915655732154846 \n",
      "At 142 epoch, Training Loss: 0.688344115763903 \n",
      "At 142 epoch, Validation Loss: 0.6915358543395996 \n",
      "At 143 epoch, Training Loss: 0.6886893492192028 \n",
      "At 143 epoch, Validation Loss: 0.6912687063217163 \n",
      "At 144 epoch, Training Loss: 0.6877977497875694 \n",
      "At 144 epoch, Validation Loss: 0.6934344500303269 \n",
      "At 145 epoch, Training Loss: 0.6866193655878305 \n",
      "At 145 epoch, Validation Loss: 0.6913769155740737 \n",
      "At 146 epoch, Training Loss: 0.6865017250180248 \n",
      "At 146 epoch, Validation Loss: 0.6916255265474319 \n",
      "At 147 epoch, Training Loss: 0.6877814680337903 \n",
      "At 147 epoch, Validation Loss: 0.6913564771413803 \n",
      "At 148 epoch, Training Loss: 0.6876107349991798 \n",
      "At 148 epoch, Validation Loss: 0.6926429212093353 \n",
      "At 149 epoch, Training Loss: 0.687278313189745 \n",
      "At 149 epoch, Validation Loss: 0.6918512135744095 \n",
      "At 150 epoch, Training Loss: 0.6890937328338622 \n",
      "At 150 epoch, Validation Loss: 0.6915417194366457 \n",
      "At 151 epoch, Training Loss: 0.685570384562015 \n",
      "At 151 epoch, Validation Loss: 0.6913104176521302 \n",
      "At 152 epoch, Training Loss: 0.6882996525615458 \n",
      "At 152 epoch, Validation Loss: 0.6935870170593262 \n",
      "At 153 epoch, Training Loss: 0.6880401127040385 \n",
      "At 153 epoch, Validation Loss: 0.6917204469442367 \n",
      "At 154 epoch, Training Loss: 0.687031835317612 \n",
      "At 154 epoch, Validation Loss: 0.6919910728931427 \n",
      "At 155 epoch, Training Loss: 0.687437672540546 \n",
      "At 155 epoch, Validation Loss: 0.6912669658660889 \n",
      "At 156 epoch, Training Loss: 0.6871463473886251 \n",
      "At 156 epoch, Validation Loss: 0.6912478119134904 \n",
      "At 157 epoch, Training Loss: 0.6876190509647132 \n",
      "At 157 epoch, Validation Loss: 0.6903855204582214 \n",
      "At 158 epoch, Training Loss: 0.688089952990413 \n",
      "At 158 epoch, Validation Loss: 0.6910066246986389 \n",
      "At 159 epoch, Training Loss: 0.6865523274987935 \n",
      "At 159 epoch, Validation Loss: 0.6916106820106506 \n",
      "At 160 epoch, Training Loss: 0.6874152991920709 \n",
      "At 160 epoch, Validation Loss: 0.6911741554737091 \n",
      "At 161 epoch, Training Loss: 0.6888791102916003 \n",
      "At 161 epoch, Validation Loss: 0.6911311894655228 \n",
      "At 162 epoch, Training Loss: 0.6878164246678352 \n",
      "At 162 epoch, Validation Loss: 0.6919163584709169 \n",
      "At 163 epoch, Training Loss: 0.686313545331359 \n",
      "At 163 epoch, Validation Loss: 0.6905250906944275 \n",
      "At 164 epoch, Training Loss: 0.6857801556587221 \n",
      "At 164 epoch, Validation Loss: 0.6917350471019745 \n",
      "At 165 epoch, Training Loss: 0.6865765921771526 \n",
      "At 165 epoch, Validation Loss: 0.6923240572214125 \n",
      "At 166 epoch, Training Loss: 0.6877661574631925 \n",
      "At 166 epoch, Validation Loss: 0.692725935578346 \n",
      "At 167 epoch, Training Loss: 0.6867883808910846 \n",
      "At 167 epoch, Validation Loss: 0.6914277523756027 \n",
      "At 168 epoch, Training Loss: 0.687644539028406 \n",
      "At 168 epoch, Validation Loss: 0.6929918736219405 \n",
      "At 169 epoch, Training Loss: 0.6878421366214751 \n",
      "At 169 epoch, Validation Loss: 0.6908052176237106 \n",
      "At 170 epoch, Training Loss: 0.6867378309369089 \n",
      "At 170 epoch, Validation Loss: 0.6911618202924729 \n",
      "At 171 epoch, Training Loss: 0.6865491624921563 \n",
      "At 171 epoch, Validation Loss: 0.6900034785270691 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 172 epoch, Training Loss: 0.6871387798339131 \n",
      "At 172 epoch, Validation Loss: 0.6907767415046693 \n",
      "At 173 epoch, Training Loss: 0.6859589021652936 \n",
      "At 173 epoch, Validation Loss: 0.6902280777692796 \n",
      "At 174 epoch, Training Loss: 0.6883457131683827 \n",
      "At 174 epoch, Validation Loss: 0.690518155694008 \n",
      "At 175 epoch, Training Loss: 0.6871982987970109 \n",
      "At 175 epoch, Validation Loss: 0.6911235451698304 \n",
      "At 176 epoch, Training Loss: 0.6863021790981294 \n",
      "At 176 epoch, Validation Loss: 0.6899845480918886 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 177 epoch, Training Loss: 0.686212442442775 \n",
      "At 177 epoch, Validation Loss: 0.6923620045185089 \n",
      "At 178 epoch, Training Loss: 0.687116260826588 \n",
      "At 178 epoch, Validation Loss: 0.6922627419233323 \n",
      "At 179 epoch, Training Loss: 0.6869980659335853 \n",
      "At 179 epoch, Validation Loss: 0.6903670638799667 \n",
      "At 180 epoch, Training Loss: 0.6874825749546288 \n",
      "At 180 epoch, Validation Loss: 0.6903414517641067 \n",
      "At 181 epoch, Training Loss: 0.6867567475885152 \n",
      "At 181 epoch, Validation Loss: 0.6913323014974593 \n",
      "At 182 epoch, Training Loss: 0.6874506548047067 \n",
      "At 182 epoch, Validation Loss: 0.6901282995939256 \n",
      "At 183 epoch, Training Loss: 0.6867848932743073 \n",
      "At 183 epoch, Validation Loss: 0.6905549168586731 \n",
      "At 184 epoch, Training Loss: 0.6855631001293656 \n",
      "At 184 epoch, Validation Loss: 0.6906831026077269 \n",
      "At 185 epoch, Training Loss: 0.6860707283020021 \n",
      "At 185 epoch, Validation Loss: 0.6905216008424759 \n",
      "At 186 epoch, Training Loss: 0.6864365827292204 \n",
      "At 186 epoch, Validation Loss: 0.6904155820608139 \n",
      "At 187 epoch, Training Loss: 0.6862674634903667 \n",
      "At 187 epoch, Validation Loss: 0.6908287823200225 \n",
      "At 188 epoch, Training Loss: 0.6865285452455285 \n",
      "At 188 epoch, Validation Loss: 0.6898137778043747 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 189 epoch, Training Loss: 0.6867203503847125 \n",
      "At 189 epoch, Validation Loss: 0.6905751883983612 \n",
      "At 190 epoch, Training Loss: 0.6877201594412327 \n",
      "At 190 epoch, Validation Loss: 0.6913920164108275 \n",
      "At 191 epoch, Training Loss: 0.6863461472094056 \n",
      "At 191 epoch, Validation Loss: 0.691311964392662 \n",
      "At 192 epoch, Training Loss: 0.6848308101296425 \n",
      "At 192 epoch, Validation Loss: 0.6914731770753861 \n",
      "At 193 epoch, Training Loss: 0.6868891984224319 \n",
      "At 193 epoch, Validation Loss: 0.6913307696580885 \n",
      "At 194 epoch, Training Loss: 0.6861028242856261 \n",
      "At 194 epoch, Validation Loss: 0.6911749005317688 \n",
      "At 195 epoch, Training Loss: 0.6855183903127909 \n",
      "At 195 epoch, Validation Loss: 0.6904732704162598 \n",
      "At 196 epoch, Training Loss: 0.6845954161137342 \n",
      "At 196 epoch, Validation Loss: 0.6905775755643845 \n",
      "At 197 epoch, Training Loss: 0.6866722501814365 \n",
      "At 197 epoch, Validation Loss: 0.6904634118080138 \n",
      "At 198 epoch, Training Loss: 0.6876242510974405 \n",
      "At 198 epoch, Validation Loss: 0.6904344886541367 \n",
      "At 199 epoch, Training Loss: 0.6839491069316865 \n",
      "At 199 epoch, Validation Loss: 0.690994617342949 \n",
      "At 200 epoch, Training Loss: 0.6867205642163752 \n",
      "At 200 epoch, Validation Loss: 0.6901553869247437 \n",
      "At 201 epoch, Training Loss: 0.6861440315842631 \n",
      "At 201 epoch, Validation Loss: 0.6895584166049955 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 202 epoch, Training Loss: 0.685089755803347 \n",
      "At 202 epoch, Validation Loss: 0.6899321019649506 \n",
      "At 203 epoch, Training Loss: 0.6865446891635657 \n",
      "At 203 epoch, Validation Loss: 0.6906870931386948 \n",
      "At 204 epoch, Training Loss: 0.6858080290257926 \n",
      "At 204 epoch, Validation Loss: 0.6903676360845566 \n",
      "At 205 epoch, Training Loss: 0.6874888349324464 \n",
      "At 205 epoch, Validation Loss: 0.6896407276391983 \n",
      "At 206 epoch, Training Loss: 0.6864449273794887 \n",
      "At 206 epoch, Validation Loss: 0.6908703744411469 \n",
      "At 207 epoch, Training Loss: 0.6854957822710277 \n",
      "At 207 epoch, Validation Loss: 0.6901280105113983 \n",
      "At 208 epoch, Training Loss: 0.686349966749549 \n",
      "At 208 epoch, Validation Loss: 0.6902920246124267 \n",
      "At 209 epoch, Training Loss: 0.6850784093141552 \n",
      "At 209 epoch, Validation Loss: 0.6897368788719177 \n",
      "At 210 epoch, Training Loss: 0.686755560338497 \n",
      "At 210 epoch, Validation Loss: 0.6905537724494935 \n",
      "At 211 epoch, Training Loss: 0.6857759814709421 \n",
      "At 211 epoch, Validation Loss: 0.6908966332674027 \n",
      "At 212 epoch, Training Loss: 0.6851256448775532 \n",
      "At 212 epoch, Validation Loss: 0.6910111874341963 \n",
      "At 213 epoch, Training Loss: 0.6870462860912083 \n",
      "At 213 epoch, Validation Loss: 0.6911617636680603 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 214 epoch, Training Loss: 0.685430125892162 \n",
      "At 214 epoch, Validation Loss: 0.6904507577419281 \n",
      "At 215 epoch, Training Loss: 0.6865765929222108 \n",
      "At 215 epoch, Validation Loss: 0.6903118550777435 \n",
      "At 216 epoch, Training Loss: 0.6851585436612366 \n",
      "At 216 epoch, Validation Loss: 0.6893782317638397 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 217 epoch, Training Loss: 0.6854408416897058 \n",
      "At 217 epoch, Validation Loss: 0.6917240977287292 \n",
      "At 218 epoch, Training Loss: 0.6867874484509233 \n",
      "At 218 epoch, Validation Loss: 0.689919701218605 \n",
      "At 219 epoch, Training Loss: 0.6871902152895925 \n",
      "At 219 epoch, Validation Loss: 0.689953711628914 \n",
      "At 220 epoch, Training Loss: 0.6853094659745694 \n",
      "At 220 epoch, Validation Loss: 0.6902247697114945 \n",
      "At 221 epoch, Training Loss: 0.6854205418378115 \n",
      "At 221 epoch, Validation Loss: 0.6898583203554154 \n",
      "At 222 epoch, Training Loss: 0.6870256748050453 \n",
      "At 222 epoch, Validation Loss: 0.6912339836359026 \n",
      "At 223 epoch, Training Loss: 0.6857752069830892 \n",
      "At 223 epoch, Validation Loss: 0.6901116311550142 \n",
      "At 224 epoch, Training Loss: 0.6858895495533944 \n",
      "At 224 epoch, Validation Loss: 0.6909864127635955 \n",
      "At 225 epoch, Training Loss: 0.6867279238998891 \n",
      "At 225 epoch, Validation Loss: 0.6912931501865387 \n",
      "At 226 epoch, Training Loss: 0.6873996037989851 \n",
      "At 226 epoch, Validation Loss: 0.6904851526021958 \n",
      "At 227 epoch, Training Loss: 0.6860060147941115 \n",
      "At 227 epoch, Validation Loss: 0.6905532598495482 \n",
      "At 228 epoch, Training Loss: 0.6841560792177918 \n",
      "At 228 epoch, Validation Loss: 0.6905015021562577 \n",
      "At 229 epoch, Training Loss: 0.6871677134186027 \n",
      "At 229 epoch, Validation Loss: 0.6908728539943695 \n",
      "At 230 epoch, Training Loss: 0.6852555893361568 \n",
      "At 230 epoch, Validation Loss: 0.6908463329076767 \n",
      "At 231 epoch, Training Loss: 0.6856483139097693 \n",
      "At 231 epoch, Validation Loss: 0.6910166651010514 \n",
      "At 232 epoch, Training Loss: 0.6873263616114855 \n",
      "At 232 epoch, Validation Loss: 0.6905750364065172 \n",
      "At 233 epoch, Training Loss: 0.6847309488803148 \n",
      "At 233 epoch, Validation Loss: 0.6917737215757372 \n",
      "At 234 epoch, Training Loss: 0.6867162309587006 \n",
      "At 234 epoch, Validation Loss: 0.6906937986612319 \n",
      "At 235 epoch, Training Loss: 0.6852332498878241 \n",
      "At 235 epoch, Validation Loss: 0.6895372867584227 \n",
      "At 236 epoch, Training Loss: 0.6869542818516492 \n",
      "At 236 epoch, Validation Loss: 0.6898069471120835 \n",
      "At 237 epoch, Training Loss: 0.684187489748001 \n",
      "At 237 epoch, Validation Loss: 0.6900381445884705 \n",
      "At 238 epoch, Training Loss: 0.6874192707240585 \n",
      "At 238 epoch, Validation Loss: 0.6892579138278961 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 239 epoch, Training Loss: 0.6862593740224839 \n",
      "At 239 epoch, Validation Loss: 0.6896293491125107 \n",
      "At 240 epoch, Training Loss: 0.6855851508677006 \n",
      "At 240 epoch, Validation Loss: 0.6907692074775698 \n",
      "At 241 epoch, Training Loss: 0.6847966831177476 \n",
      "At 241 epoch, Validation Loss: 0.6928419023752214 \n",
      "At 242 epoch, Training Loss: 0.6862953376024961 \n",
      "At 242 epoch, Validation Loss: 0.6900048941373824 \n",
      "At 243 epoch, Training Loss: 0.6865714699029922 \n",
      "At 243 epoch, Validation Loss: 0.689363059401512 \n",
      "At 244 epoch, Training Loss: 0.6863122239708896 \n",
      "At 244 epoch, Validation Loss: 0.6903130829334259 \n",
      "At 245 epoch, Training Loss: 0.6859785519540306 \n",
      "At 245 epoch, Validation Loss: 0.6894491314887999 \n",
      "At 246 epoch, Training Loss: 0.6858027461916205 \n",
      "At 246 epoch, Validation Loss: 0.6905487358570099 \n",
      "At 247 epoch, Training Loss: 0.6866379342973233 \n",
      "At 247 epoch, Validation Loss: 0.6902747690677642 \n",
      "At 248 epoch, Training Loss: 0.6870361417531967 \n",
      "At 248 epoch, Validation Loss: 0.6909718573093415 \n",
      "At 249 epoch, Training Loss: 0.685484978184104 \n",
      "At 249 epoch, Validation Loss: 0.6910728186368944 \n",
      "At 250 epoch, Training Loss: 0.6862019125372169 \n",
      "At 250 epoch, Validation Loss: 0.6902018845081328 \n",
      "At 251 epoch, Training Loss: 0.6860489830374717 \n",
      "At 251 epoch, Validation Loss: 0.690947660803795 \n",
      "At 252 epoch, Training Loss: 0.6864775110036135 \n",
      "At 252 epoch, Validation Loss: 0.6905263394117355 \n",
      "At 253 epoch, Training Loss: 0.6864145442843432 \n",
      "At 253 epoch, Validation Loss: 0.6910848587751387 \n",
      "At 254 epoch, Training Loss: 0.6846430372446775 \n",
      "At 254 epoch, Validation Loss: 0.6910970628261567 \n",
      "At 255 epoch, Training Loss: 0.6854009624570612 \n",
      "At 255 epoch, Validation Loss: 0.6906276166439057 \n",
      "At 256 epoch, Training Loss: 0.6838964879512789 \n",
      "At 256 epoch, Validation Loss: 0.6914009481668473 \n",
      "At 257 epoch, Training Loss: 0.6857203323394062 \n",
      "At 257 epoch, Validation Loss: 0.6910431563854219 \n",
      "At 258 epoch, Training Loss: 0.6857344571501017 \n",
      "At 258 epoch, Validation Loss: 0.6902995109558105 \n",
      "At 259 epoch, Training Loss: 0.6869784850627182 \n",
      "At 259 epoch, Validation Loss: 0.6902944445610047 \n",
      "At 260 epoch, Training Loss: 0.6858186565339567 \n",
      "At 260 epoch, Validation Loss: 0.69020716547966 \n",
      "At 261 epoch, Training Loss: 0.6861533898860216 \n",
      "At 261 epoch, Validation Loss: 0.6904813647270203 \n",
      "At 262 epoch, Training Loss: 0.6853423703461888 \n",
      "At 262 epoch, Validation Loss: 0.6896045118570328 \n",
      "At 263 epoch, Training Loss: 0.6855547908693557 \n",
      "At 263 epoch, Validation Loss: 0.6901601821184159 \n",
      "At 264 epoch, Training Loss: 0.6864475343376398 \n",
      "At 264 epoch, Validation Loss: 0.690148389339447 \n",
      "At 265 epoch, Training Loss: 0.6868486754596229 \n",
      "At 265 epoch, Validation Loss: 0.6915215402841568 \n",
      "At 266 epoch, Training Loss: 0.6853880982846022 \n",
      "At 266 epoch, Validation Loss: 0.690726387500763 \n",
      "At 267 epoch, Training Loss: 0.6847343232482668 \n",
      "At 267 epoch, Validation Loss: 0.6929512679576874 \n",
      "At 268 epoch, Training Loss: 0.6857056163251397 \n",
      "At 268 epoch, Validation Loss: 0.6910618782043456 \n",
      "At 269 epoch, Training Loss: 0.6848091032356023 \n",
      "At 269 epoch, Validation Loss: 0.6913500577211381 \n",
      "At 270 epoch, Training Loss: 0.6846892278641461 \n",
      "At 270 epoch, Validation Loss: 0.6908648073673247 \n",
      "At 271 epoch, Training Loss: 0.6844870969653135 \n",
      "At 271 epoch, Validation Loss: 0.6903552234172822 \n",
      "At 272 epoch, Training Loss: 0.6847227580845356 \n",
      "At 272 epoch, Validation Loss: 0.6906335860490799 \n",
      "At 273 epoch, Training Loss: 0.6877406928688284 \n",
      "At 273 epoch, Validation Loss: 0.6905215978622438 \n",
      "At 274 epoch, Training Loss: 0.6843394182622435 \n",
      "At 274 epoch, Validation Loss: 0.6900646477937699 \n",
      "At 275 epoch, Training Loss: 0.6850161239504813 \n",
      "At 275 epoch, Validation Loss: 0.6901899218559264 \n",
      "At 276 epoch, Training Loss: 0.6849131066352128 \n",
      "At 276 epoch, Validation Loss: 0.6912946403026583 \n",
      "At 277 epoch, Training Loss: 0.6848142836242915 \n",
      "At 277 epoch, Validation Loss: 0.6909165382385254 \n",
      "At 278 epoch, Training Loss: 0.6862931285053494 \n",
      "At 278 epoch, Validation Loss: 0.6900383174419403 \n",
      "At 279 epoch, Training Loss: 0.6841044936329123 \n",
      "At 279 epoch, Validation Loss: 0.6915259629487992 \n",
      "At 280 epoch, Training Loss: 0.6868163805454963 \n",
      "At 280 epoch, Validation Loss: 0.6911542922258378 \n",
      "At 281 epoch, Training Loss: 0.6854888617992398 \n",
      "At 281 epoch, Validation Loss: 0.6903048247098923 \n",
      "At 282 epoch, Training Loss: 0.6840934798121451 \n",
      "At 282 epoch, Validation Loss: 0.6909908741712572 \n",
      "At 283 epoch, Training Loss: 0.6857309967279436 \n",
      "At 283 epoch, Validation Loss: 0.6926252037286758 \n",
      "At 284 epoch, Training Loss: 0.6858010500669486 \n",
      "At 284 epoch, Validation Loss: 0.690400278568268 \n",
      "At 285 epoch, Training Loss: 0.6847565129399298 \n",
      "At 285 epoch, Validation Loss: 0.6912933439016342 \n",
      "At 286 epoch, Training Loss: 0.6852259263396261 \n",
      "At 286 epoch, Validation Loss: 0.6895935088396072 \n",
      "At 287 epoch, Training Loss: 0.6854245547205207 \n",
      "At 287 epoch, Validation Loss: 0.6908832848072052 \n",
      "At 288 epoch, Training Loss: 0.6859672017395498 \n",
      "At 288 epoch, Validation Loss: 0.6893164068460464 \n",
      "At 289 epoch, Training Loss: 0.6843696154654025 \n",
      "At 289 epoch, Validation Loss: 0.6904640465974806 \n",
      "At 290 epoch, Training Loss: 0.6866641599684951 \n",
      "At 290 epoch, Validation Loss: 0.6899112462997437 \n",
      "At 291 epoch, Training Loss: 0.6862015642225737 \n",
      "At 291 epoch, Validation Loss: 0.6895062208175659 \n",
      "At 292 epoch, Training Loss: 0.6850659370422364 \n",
      "At 292 epoch, Validation Loss: 0.6905552417039871 \n",
      "At 293 epoch, Training Loss: 0.6846057485789057 \n",
      "At 293 epoch, Validation Loss: 0.6904061943292619 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 294 epoch, Training Loss: 0.6852658186107873 \n",
      "At 294 epoch, Validation Loss: 0.6900533616542815 \n",
      "At 295 epoch, Training Loss: 0.685781967639923 \n",
      "At 295 epoch, Validation Loss: 0.6906825512647629 \n",
      "At 296 epoch, Training Loss: 0.6845777604728939 \n",
      "At 296 epoch, Validation Loss: 0.6887775987386704 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 297 epoch, Training Loss: 0.6840450141578912 \n",
      "At 297 epoch, Validation Loss: 0.6900669664144516 \n",
      "At 298 epoch, Training Loss: 0.6848349954932929 \n",
      "At 298 epoch, Validation Loss: 0.6904726475477219 \n",
      "At 299 epoch, Training Loss: 0.6853152345865966 \n",
      "At 299 epoch, Validation Loss: 0.6900989174842834 \n",
      "At 300 epoch, Training Loss: 0.6855975378304717 \n",
      "At 300 epoch, Validation Loss: 0.6889995634555816 \n",
      "At 301 epoch, Training Loss: 0.6858128324151042 \n",
      "At 301 epoch, Validation Loss: 0.6890704631805419 \n",
      "At 302 epoch, Training Loss: 0.6864399261772631 \n",
      "At 302 epoch, Validation Loss: 0.6888369560241697 \n",
      "At 303 epoch, Training Loss: 0.6837684147059913 \n",
      "At 303 epoch, Validation Loss: 0.6893855303525925 \n",
      "At 304 epoch, Training Loss: 0.6855706889182329 \n",
      "At 304 epoch, Validation Loss: 0.6899519473314285 \n",
      "At 305 epoch, Training Loss: 0.6844285324215887 \n",
      "At 305 epoch, Validation Loss: 0.6898491114377975 \n",
      "At 306 epoch, Training Loss: 0.6846614010632038 \n",
      "At 306 epoch, Validation Loss: 0.6900301873683928 \n",
      "At 307 epoch, Training Loss: 0.6854008700698608 \n",
      "At 307 epoch, Validation Loss: 0.6896572828292846 \n",
      "At 308 epoch, Training Loss: 0.6856875792145729 \n",
      "At 308 epoch, Validation Loss: 0.6893543004989622 \n",
      "At 309 epoch, Training Loss: 0.6858956668525933 \n",
      "At 309 epoch, Validation Loss: 0.6895801573991776 \n",
      "At 310 epoch, Training Loss: 0.6856297351419924 \n",
      "At 310 epoch, Validation Loss: 0.6904352009296416 \n",
      "At 311 epoch, Training Loss: 0.6853487461805345 \n",
      "At 311 epoch, Validation Loss: 0.6907227307558059 \n",
      "At 312 epoch, Training Loss: 0.6858281426131725 \n",
      "At 312 epoch, Validation Loss: 0.6897717982530593 \n",
      "At 313 epoch, Training Loss: 0.684385957941413 \n",
      "At 313 epoch, Validation Loss: 0.6905796229839325 \n",
      "At 314 epoch, Training Loss: 0.6859910618513823 \n",
      "At 314 epoch, Validation Loss: 0.6897599041461944 \n",
      "At 315 epoch, Training Loss: 0.6841807980090383 \n",
      "At 315 epoch, Validation Loss: 0.6899079352617264 \n",
      "At 316 epoch, Training Loss: 0.6847366426140068 \n",
      "At 316 epoch, Validation Loss: 0.6894780158996582 \n",
      "At 317 epoch, Training Loss: 0.6853068374097346 \n",
      "At 317 epoch, Validation Loss: 0.6890058130025862 \n",
      "At 318 epoch, Training Loss: 0.684753955900669 \n",
      "At 318 epoch, Validation Loss: 0.689890569448471 \n",
      "At 319 epoch, Training Loss: 0.683985745161772 \n",
      "At 319 epoch, Validation Loss: 0.6893060177564619 \n",
      "At 320 epoch, Training Loss: 0.6857515849173069 \n",
      "At 320 epoch, Validation Loss: 0.6889751374721526 \n",
      "At 321 epoch, Training Loss: 0.6836901202797891 \n",
      "At 321 epoch, Validation Loss: 0.6907229036092758 \n",
      "At 322 epoch, Training Loss: 0.6843655698001387 \n",
      "At 322 epoch, Validation Loss: 0.6904121607542038 \n",
      "At 323 epoch, Training Loss: 0.6847584083676337 \n",
      "At 323 epoch, Validation Loss: 0.6901355922222138 \n",
      "At 324 epoch, Training Loss: 0.6853385277092454 \n",
      "At 324 epoch, Validation Loss: 0.6902413785457612 \n",
      "At 325 epoch, Training Loss: 0.6841352872550491 \n",
      "At 325 epoch, Validation Loss: 0.6888313531875611 \n",
      "At 326 epoch, Training Loss: 0.6856956843286753 \n",
      "At 326 epoch, Validation Loss: 0.691395452618599 \n",
      "At 327 epoch, Training Loss: 0.6845011580735443 \n",
      "At 327 epoch, Validation Loss: 0.6904880195856093 \n",
      "At 328 epoch, Training Loss: 0.6845896117389201 \n",
      "At 328 epoch, Validation Loss: 0.6907452881336211 \n",
      "At 329 epoch, Training Loss: 0.6842312879860403 \n",
      "At 329 epoch, Validation Loss: 0.6884965300559998 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 330 epoch, Training Loss: 0.6851112335920334 \n",
      "At 330 epoch, Validation Loss: 0.6882719546556472 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 331 epoch, Training Loss: 0.6836679659783843 \n",
      "At 331 epoch, Validation Loss: 0.6894876062870026 \n",
      "At 332 epoch, Training Loss: 0.6830958340317006 \n",
      "At 332 epoch, Validation Loss: 0.6889709502458573 \n",
      "At 333 epoch, Training Loss: 0.6830344248563054 \n",
      "At 333 epoch, Validation Loss: 0.6906881719827652 \n",
      "At 334 epoch, Training Loss: 0.6856953527778386 \n",
      "At 334 epoch, Validation Loss: 0.6881773263216018 \n",
      "Minimum validation loss detected, saving model......................................................................................\n",
      "At 335 epoch, Training Loss: 0.6867875147610907 \n",
      "At 335 epoch, Validation Loss: 0.6891840904951095 \n",
      "At 336 epoch, Training Loss: 0.6842401754111044 \n",
      "At 336 epoch, Validation Loss: 0.6898542761802673 \n",
      "At 337 epoch, Training Loss: 0.6849749989807608 \n",
      "At 337 epoch, Validation Loss: 0.6888687729835511 \n",
      "At 338 epoch, Training Loss: 0.6846705909818412 \n",
      "At 338 epoch, Validation Loss: 0.6886286079883576 \n",
      "At 339 epoch, Training Loss: 0.6868299663066862 \n",
      "At 339 epoch, Validation Loss: 0.6893481373786926 \n",
      "At 340 epoch, Training Loss: 0.6871125508099796 \n",
      "At 340 epoch, Validation Loss: 0.689562639594078 \n",
      "At 341 epoch, Training Loss: 0.6846531376242637 \n",
      "At 341 epoch, Validation Loss: 0.6903288424015046 \n",
      "At 342 epoch, Training Loss: 0.6833761148154734 \n",
      "At 342 epoch, Validation Loss: 0.6899597078561783 \n",
      "At 343 epoch, Training Loss: 0.6852128613740203 \n",
      "At 343 epoch, Validation Loss: 0.689663118124008 \n",
      "At 344 epoch, Training Loss: 0.685723264142871 \n",
      "At 344 epoch, Validation Loss: 0.6889425039291381 \n",
      "At 345 epoch, Training Loss: 0.6851071968674661 \n",
      "At 345 epoch, Validation Loss: 0.6909291476011276 \n",
      "At 346 epoch, Training Loss: 0.6834572546184063 \n",
      "At 346 epoch, Validation Loss: 0.6900593459606172 \n",
      "At 347 epoch, Training Loss: 0.6826239295303819 \n",
      "At 347 epoch, Validation Loss: 0.6894032567739486 \n",
      "At 348 epoch, Training Loss: 0.6825319241732359 \n",
      "At 348 epoch, Validation Loss: 0.6895475208759309 \n",
      "At 349 epoch, Training Loss: 0.683782725408673 \n",
      "At 349 epoch, Validation Loss: 0.689566433429718 \n",
      "At 350 epoch, Training Loss: 0.6850012149661779 \n",
      "At 350 epoch, Validation Loss: 0.6891611456871033 \n",
      "At 351 epoch, Training Loss: 0.6848865922540427 \n",
      "At 351 epoch, Validation Loss: 0.6896209150552749 \n",
      "At 352 epoch, Training Loss: 0.6854949630796905 \n",
      "At 352 epoch, Validation Loss: 0.6897011756896971 \n",
      "At 353 epoch, Training Loss: 0.6835340514779088 \n",
      "At 353 epoch, Validation Loss: 0.6899175882339479 \n",
      "At 354 epoch, Training Loss: 0.6820837896317242 \n",
      "At 354 epoch, Validation Loss: 0.6900791764259337 \n",
      "At 355 epoch, Training Loss: 0.685130653902889 \n",
      "At 355 epoch, Validation Loss: 0.6896127700805664 \n",
      "At 356 epoch, Training Loss: 0.6836821749806404 \n",
      "At 356 epoch, Validation Loss: 0.6896582961082458 \n",
      "At 357 epoch, Training Loss: 0.6844472814351319 \n",
      "At 357 epoch, Validation Loss: 0.6893678575754165 \n",
      "At 358 epoch, Training Loss: 0.6848940633237361 \n",
      "At 358 epoch, Validation Loss: 0.6899675011634827 \n",
      "At 359 epoch, Training Loss: 0.685353123396635 \n",
      "At 359 epoch, Validation Loss: 0.690424683690071 \n",
      "At 360 epoch, Training Loss: 0.6866977326571941 \n",
      "At 360 epoch, Validation Loss: 0.6906125843524932 \n",
      "At 361 epoch, Training Loss: 0.6840522084385157 \n",
      "At 361 epoch, Validation Loss: 0.6902200907468795 \n",
      "At 362 epoch, Training Loss: 0.684811708331108 \n",
      "At 362 epoch, Validation Loss: 0.6905828714370728 \n",
      "At 363 epoch, Training Loss: 0.6844767965376376 \n",
      "At 363 epoch, Validation Loss: 0.6901741445064546 \n",
      "At 364 epoch, Training Loss: 0.6826345346868038 \n",
      "At 364 epoch, Validation Loss: 0.6904056251049041 \n",
      "At 365 epoch, Training Loss: 0.6860063694417475 \n",
      "At 365 epoch, Validation Loss: 0.6907016694545747 \n",
      "At 366 epoch, Training Loss: 0.6848128009587529 \n",
      "At 366 epoch, Validation Loss: 0.6912562906742095 \n",
      "At 367 epoch, Training Loss: 0.6843635857105254 \n",
      "At 367 epoch, Validation Loss: 0.6903911411762238 \n",
      "At 368 epoch, Training Loss: 0.6866125214844936 \n",
      "At 368 epoch, Validation Loss: 0.6899945020675657 \n",
      "At 369 epoch, Training Loss: 0.6852472815662627 \n",
      "At 369 epoch, Validation Loss: 0.6902719855308532 \n",
      "At 370 epoch, Training Loss: 0.6844380542635915 \n",
      "At 370 epoch, Validation Loss: 0.6903752833604812 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 371 epoch, Training Loss: 0.6850590649992225 \n",
      "At 371 epoch, Validation Loss: 0.6902014046907424 \n",
      "At 372 epoch, Training Loss: 0.6841400660574442 \n",
      "At 372 epoch, Validation Loss: 0.6912703186273577 \n",
      "At 373 epoch, Training Loss: 0.684273881837726 \n",
      "At 373 epoch, Validation Loss: 0.6927472710609436 \n",
      "At 374 epoch, Training Loss: 0.6831819787621497 \n",
      "At 374 epoch, Validation Loss: 0.6910486906766892 \n",
      "At 375 epoch, Training Loss: 0.6835588783025742 \n",
      "At 375 epoch, Validation Loss: 0.6899774610996247 \n",
      "At 376 epoch, Training Loss: 0.6857758153229951 \n",
      "At 376 epoch, Validation Loss: 0.6896387070417406 \n",
      "At 377 epoch, Training Loss: 0.6841124165803194 \n",
      "At 377 epoch, Validation Loss: 0.690736624598503 \n",
      "At 378 epoch, Training Loss: 0.6834895741194488 \n",
      "At 378 epoch, Validation Loss: 0.6900037974119188 \n",
      "At 379 epoch, Training Loss: 0.6848978895694019 \n",
      "At 379 epoch, Validation Loss: 0.6899360477924346 \n",
      "At 380 epoch, Training Loss: 0.6840698312968015 \n",
      "At 380 epoch, Validation Loss: 0.6895460665225982 \n",
      "At 381 epoch, Training Loss: 0.6855647258460523 \n",
      "At 381 epoch, Validation Loss: 0.6907854318618774 \n",
      "At 382 epoch, Training Loss: 0.6847845211625101 \n",
      "At 382 epoch, Validation Loss: 0.6891438215970992 \n",
      "At 383 epoch, Training Loss: 0.6841739814728502 \n",
      "At 383 epoch, Validation Loss: 0.6890482962131501 \n",
      "At 384 epoch, Training Loss: 0.6843118250370024 \n",
      "At 384 epoch, Validation Loss: 0.6897619277238844 \n",
      "At 385 epoch, Training Loss: 0.6846927661448716 \n",
      "At 385 epoch, Validation Loss: 0.6907003134489059 \n",
      "At 386 epoch, Training Loss: 0.6848682191222906 \n",
      "At 386 epoch, Validation Loss: 0.6904996722936629 \n",
      "At 387 epoch, Training Loss: 0.6842902868986125 \n",
      "At 387 epoch, Validation Loss: 0.6897684514522553 \n",
      "At 388 epoch, Training Loss: 0.6835552763193845 \n",
      "At 388 epoch, Validation Loss: 0.6893872648477555 \n",
      "At 389 epoch, Training Loss: 0.6853254634886979 \n",
      "At 389 epoch, Validation Loss: 0.6895141243934633 \n",
      "At 390 epoch, Training Loss: 0.6846376936882735 \n",
      "At 390 epoch, Validation Loss: 0.6898144066333771 \n",
      "At 391 epoch, Training Loss: 0.682556454092264 \n",
      "At 391 epoch, Validation Loss: 0.6901615053415298 \n",
      "At 392 epoch, Training Loss: 0.6861899949610234 \n",
      "At 392 epoch, Validation Loss: 0.6903109610080721 \n",
      "At 393 epoch, Training Loss: 0.6844186671078201 \n",
      "At 393 epoch, Validation Loss: 0.6900935262441634 \n",
      "At 394 epoch, Training Loss: 0.6843690861016515 \n",
      "At 394 epoch, Validation Loss: 0.690440845489502 \n",
      "At 395 epoch, Training Loss: 0.684658670052886 \n",
      "At 395 epoch, Validation Loss: 0.6905612617731093 \n",
      "At 396 epoch, Training Loss: 0.684950014948845 \n",
      "At 396 epoch, Validation Loss: 0.6903896033763885 \n",
      "At 397 epoch, Training Loss: 0.6840249005705115 \n",
      "At 397 epoch, Validation Loss: 0.6892936766147615 \n",
      "At 398 epoch, Training Loss: 0.6849546499550347 \n",
      "At 398 epoch, Validation Loss: 0.6894124329090118 \n",
      "At 399 epoch, Training Loss: 0.6826484192162751 \n",
      "At 399 epoch, Validation Loss: 0.6894544541835786 \n",
      "At 400 epoch, Training Loss: 0.6834874395281079 \n",
      "At 400 epoch, Validation Loss: 0.6902806967496872 \n",
      "At 401 epoch, Training Loss: 0.6840876538306473 \n",
      "At 401 epoch, Validation Loss: 0.6905357360839843 \n",
      "At 402 epoch, Training Loss: 0.6857867218554022 \n",
      "At 402 epoch, Validation Loss: 0.6905899435281754 \n",
      "At 403 epoch, Training Loss: 0.6852885764092206 \n",
      "At 403 epoch, Validation Loss: 0.6888788342475891 \n",
      "At 404 epoch, Training Loss: 0.6833571173250672 \n",
      "At 404 epoch, Validation Loss: 0.6897980660200118 \n",
      "At 405 epoch, Training Loss: 0.6839386522769927 \n",
      "At 405 epoch, Validation Loss: 0.6901648312807084 \n",
      "At 406 epoch, Training Loss: 0.6842918328940868 \n",
      "At 406 epoch, Validation Loss: 0.6894159585237504 \n",
      "At 407 epoch, Training Loss: 0.6826116759330036 \n",
      "At 407 epoch, Validation Loss: 0.6895269364118577 \n",
      "At 408 epoch, Training Loss: 0.685459376499057 \n",
      "At 408 epoch, Validation Loss: 0.6887532114982605 \n",
      "At 409 epoch, Training Loss: 0.6843681514263156 \n",
      "At 409 epoch, Validation Loss: 0.6894197940826416 \n",
      "At 410 epoch, Training Loss: 0.68432356454432 \n",
      "At 410 epoch, Validation Loss: 0.68938347697258 \n",
      "At 411 epoch, Training Loss: 0.6850709058344361 \n",
      "At 411 epoch, Validation Loss: 0.6894357889890671 \n",
      "At 412 epoch, Training Loss: 0.6844358846545217 \n",
      "At 412 epoch, Validation Loss: 0.6894925504922867 \n",
      "At 413 epoch, Training Loss: 0.6837657134979962 \n",
      "At 413 epoch, Validation Loss: 0.6895016461610795 \n",
      "At 414 epoch, Training Loss: 0.6842096287757158 \n",
      "At 414 epoch, Validation Loss: 0.6889990329742431 \n",
      "At 415 epoch, Training Loss: 0.6826236836612223 \n",
      "At 415 epoch, Validation Loss: 0.6892647206783294 \n",
      "At 416 epoch, Training Loss: 0.6840060517191889 \n",
      "At 416 epoch, Validation Loss: 0.6899020463228226 \n",
      "At 417 epoch, Training Loss: 0.6830688044428824 \n",
      "At 417 epoch, Validation Loss: 0.6899914413690565 \n",
      "At 418 epoch, Training Loss: 0.6847186349332334 \n",
      "At 418 epoch, Validation Loss: 0.6894547969102859 \n",
      "At 419 epoch, Training Loss: 0.6846432954072951 \n",
      "At 419 epoch, Validation Loss: 0.6896876215934755 \n",
      "At 420 epoch, Training Loss: 0.6827332869172098 \n",
      "At 420 epoch, Validation Loss: 0.6904270589351654 \n",
      "At 421 epoch, Training Loss: 0.6840998530387881 \n",
      "At 421 epoch, Validation Loss: 0.689460438489914 \n",
      "At 422 epoch, Training Loss: 0.6844294849783178 \n",
      "At 422 epoch, Validation Loss: 0.6895147591829299 \n",
      "At 423 epoch, Training Loss: 0.6852192245423793 \n",
      "At 423 epoch, Validation Loss: 0.6893249809741973 \n",
      "At 424 epoch, Training Loss: 0.6833503197878599 \n",
      "At 424 epoch, Validation Loss: 0.6903039664030074 \n",
      "At 425 epoch, Training Loss: 0.6827847156673669 \n",
      "At 425 epoch, Validation Loss: 0.6902010351419449 \n",
      "At 426 epoch, Training Loss: 0.6872679751366376 \n",
      "At 426 epoch, Validation Loss: 0.6906283378601075 \n",
      "At 427 epoch, Training Loss: 0.6849685676395896 \n",
      "At 427 epoch, Validation Loss: 0.6900907725095748 \n",
      "At 428 epoch, Training Loss: 0.685312332585454 \n",
      "At 428 epoch, Validation Loss: 0.6895869076251984 \n",
      "At 429 epoch, Training Loss: 0.6853050176054236 \n",
      "At 429 epoch, Validation Loss: 0.6902328670024871 \n",
      "At 430 epoch, Training Loss: 0.6833721239119764 \n",
      "At 430 epoch, Validation Loss: 0.6900604248046875 \n",
      "At 431 epoch, Training Loss: 0.6861581351608038 \n",
      "At 431 epoch, Validation Loss: 0.6896723240613937 \n",
      "At 432 epoch, Training Loss: 0.6859579022973777 \n",
      "At 432 epoch, Validation Loss: 0.690514925122261 \n",
      "At 433 epoch, Training Loss: 0.6839927211403848 \n",
      "At 433 epoch, Validation Loss: 0.6905898213386535 \n",
      "At 434 epoch, Training Loss: 0.6849108919501304 \n",
      "At 434 epoch, Validation Loss: 0.6907023400068284 \n",
      "At 435 epoch, Training Loss: 0.6843335781246422 \n",
      "At 435 epoch, Validation Loss: 0.6913573443889617 \n",
      "At 436 epoch, Training Loss: 0.683502301573753 \n",
      "At 436 epoch, Validation Loss: 0.6909457623958588 \n",
      "At 437 epoch, Training Loss: 0.6848483994603156 \n",
      "At 437 epoch, Validation Loss: 0.6902709633111953 \n",
      "At 438 epoch, Training Loss: 0.6833331402391198 \n",
      "At 438 epoch, Validation Loss: 0.6900514483451844 \n",
      "At 439 epoch, Training Loss: 0.6840307746082549 \n",
      "At 439 epoch, Validation Loss: 0.6901309430599213 \n",
      "At 440 epoch, Training Loss: 0.6837891377508643 \n",
      "At 440 epoch, Validation Loss: 0.6903458565473556 \n",
      "At 441 epoch, Training Loss: 0.6834385070949791 \n",
      "At 441 epoch, Validation Loss: 0.690519493818283 \n",
      "At 442 epoch, Training Loss: 0.6832687985152004 \n",
      "At 442 epoch, Validation Loss: 0.6904090344905855 \n",
      "At 443 epoch, Training Loss: 0.6845804776996371 \n",
      "At 443 epoch, Validation Loss: 0.6901599824428559 \n",
      "At 444 epoch, Training Loss: 0.6831451404839753 \n",
      "At 444 epoch, Validation Loss: 0.6901109606027602 \n",
      "At 445 epoch, Training Loss: 0.6852931708097455 \n",
      "At 445 epoch, Validation Loss: 0.6896620601415635 \n",
      "At 446 epoch, Training Loss: 0.6832714077085257 \n",
      "At 446 epoch, Validation Loss: 0.6901785522699355 \n",
      "At 447 epoch, Training Loss: 0.6830988720059394 \n",
      "At 447 epoch, Validation Loss: 0.6896654754877091 \n",
      "At 448 epoch, Training Loss: 0.6832565966993572 \n",
      "At 448 epoch, Validation Loss: 0.6895601063966751 \n",
      "At 449 epoch, Training Loss: 0.6831961635500191 \n",
      "At 449 epoch, Validation Loss: 0.6903380513191224 \n",
      "At 450 epoch, Training Loss: 0.6832306727766989 \n",
      "At 450 epoch, Validation Loss: 0.6897350311279298 \n",
      "At 451 epoch, Training Loss: 0.6826957225799557 \n",
      "At 451 epoch, Validation Loss: 0.6906911760568619 \n",
      "At 452 epoch, Training Loss: 0.6819675233215093 \n",
      "At 452 epoch, Validation Loss: 0.6918448507785798 \n",
      "At 453 epoch, Training Loss: 0.683826331049204 \n",
      "At 453 epoch, Validation Loss: 0.6898714482784271 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 454 epoch, Training Loss: 0.6843411348760126 \n",
      "At 454 epoch, Validation Loss: 0.6905455827713013 \n",
      "At 455 epoch, Training Loss: 0.6819324638694523 \n",
      "At 455 epoch, Validation Loss: 0.6908840417861939 \n",
      "At 456 epoch, Training Loss: 0.6823311783373354 \n",
      "At 456 epoch, Validation Loss: 0.6917145282030106 \n",
      "At 457 epoch, Training Loss: 0.6840764116495844 \n",
      "At 457 epoch, Validation Loss: 0.6908476263284684 \n",
      "At 458 epoch, Training Loss: 0.6833333857357505 \n",
      "At 458 epoch, Validation Loss: 0.6913846433162689 \n",
      "At 459 epoch, Training Loss: 0.6831945732235907 \n",
      "At 459 epoch, Validation Loss: 0.6900852948427201 \n",
      "At 460 epoch, Training Loss: 0.6814248420298101 \n",
      "At 460 epoch, Validation Loss: 0.6905754625797271 \n",
      "At 461 epoch, Training Loss: 0.6843293413519858 \n",
      "At 461 epoch, Validation Loss: 0.6914922654628753 \n",
      "At 462 epoch, Training Loss: 0.6828332524746659 \n",
      "At 462 epoch, Validation Loss: 0.6903480321168899 \n",
      "At 463 epoch, Training Loss: 0.6829685252159832 \n",
      "At 463 epoch, Validation Loss: 0.6909608840942384 \n",
      "At 464 epoch, Training Loss: 0.68527355901897 \n",
      "At 464 epoch, Validation Loss: 0.6907227993011475 \n",
      "At 465 epoch, Training Loss: 0.6844493318349121 \n",
      "At 465 epoch, Validation Loss: 0.6910762935876845 \n",
      "At 466 epoch, Training Loss: 0.68266374990344 \n",
      "At 466 epoch, Validation Loss: 0.6905261069536209 \n",
      "At 467 epoch, Training Loss: 0.6840526293963195 \n",
      "At 467 epoch, Validation Loss: 0.6904981166124344 \n",
      "At 468 epoch, Training Loss: 0.6824896108359099 \n",
      "At 468 epoch, Validation Loss: 0.6910388916730882 \n",
      "At 469 epoch, Training Loss: 0.6846128329634669 \n",
      "At 469 epoch, Validation Loss: 0.6901935487985612 \n",
      "At 470 epoch, Training Loss: 0.6825727645307782 \n",
      "At 470 epoch, Validation Loss: 0.6901428729295731 \n",
      "At 471 epoch, Training Loss: 0.6843425013124943 \n",
      "At 471 epoch, Validation Loss: 0.6903692066669463 \n",
      "At 472 epoch, Training Loss: 0.6846059694886205 \n",
      "At 472 epoch, Validation Loss: 0.6901812732219695 \n",
      "At 473 epoch, Training Loss: 0.6838720362633465 \n",
      "At 473 epoch, Validation Loss: 0.6900321453809739 \n",
      "At 474 epoch, Training Loss: 0.6838080689311031 \n",
      "At 474 epoch, Validation Loss: 0.6897618174552917 \n",
      "At 475 epoch, Training Loss: 0.681988199055195 \n",
      "At 475 epoch, Validation Loss: 0.6908682614564896 \n",
      "At 476 epoch, Training Loss: 0.6828732281923292 \n",
      "At 476 epoch, Validation Loss: 0.6900741577148438 \n",
      "At 477 epoch, Training Loss: 0.6851432602852583 \n",
      "At 477 epoch, Validation Loss: 0.69015554189682 \n",
      "At 478 epoch, Training Loss: 0.6829232875257736 \n",
      "At 478 epoch, Validation Loss: 0.6902900189161301 \n",
      "At 479 epoch, Training Loss: 0.6835289295762779 \n",
      "At 479 epoch, Validation Loss: 0.6899736791849137 \n",
      "At 480 epoch, Training Loss: 0.6853534623980522 \n",
      "At 480 epoch, Validation Loss: 0.6893502384424209 \n",
      "At 481 epoch, Training Loss: 0.6826425187289715 \n",
      "At 481 epoch, Validation Loss: 0.6903839528560639 \n",
      "At 482 epoch, Training Loss: 0.6823561139404773 \n",
      "At 482 epoch, Validation Loss: 0.690581738948822 \n",
      "At 483 epoch, Training Loss: 0.6815697215497494 \n",
      "At 483 epoch, Validation Loss: 0.688954770565033 \n",
      "At 484 epoch, Training Loss: 0.684083953499794 \n",
      "At 484 epoch, Validation Loss: 0.6896214485168457 \n",
      "At 485 epoch, Training Loss: 0.6815523229539393 \n",
      "At 485 epoch, Validation Loss: 0.6895299255847931 \n",
      "At 486 epoch, Training Loss: 0.6830382145941258 \n",
      "At 486 epoch, Validation Loss: 0.6903238803148269 \n",
      "At 487 epoch, Training Loss: 0.6833606116473679 \n",
      "At 487 epoch, Validation Loss: 0.6900062769651414 \n",
      "At 488 epoch, Training Loss: 0.6851437762379645 \n",
      "At 488 epoch, Validation Loss: 0.690228047966957 \n",
      "At 489 epoch, Training Loss: 0.68220206387341 \n",
      "At 489 epoch, Validation Loss: 0.6907358765602112 \n",
      "At 490 epoch, Training Loss: 0.6831039741635323 \n",
      "At 490 epoch, Validation Loss: 0.689930060505867 \n",
      "At 491 epoch, Training Loss: 0.684337066859007 \n",
      "At 491 epoch, Validation Loss: 0.6901157736778261 \n",
      "At 492 epoch, Training Loss: 0.6839851628988982 \n",
      "At 492 epoch, Validation Loss: 0.6900507271289825 \n",
      "At 493 epoch, Training Loss: 0.6814173996448516 \n",
      "At 493 epoch, Validation Loss: 0.6903215199708937 \n",
      "At 494 epoch, Training Loss: 0.6820241846144198 \n",
      "At 494 epoch, Validation Loss: 0.690258803963661 \n",
      "At 495 epoch, Training Loss: 0.6814164564013482 \n",
      "At 495 epoch, Validation Loss: 0.690571540594101 \n",
      "At 496 epoch, Training Loss: 0.682080917805433 \n",
      "At 496 epoch, Validation Loss: 0.6895903706550597 \n",
      "At 497 epoch, Training Loss: 0.6822163447737695 \n",
      "At 497 epoch, Validation Loss: 0.690063852071762 \n",
      "At 498 epoch, Training Loss: 0.6837780434638259 \n",
      "At 498 epoch, Validation Loss: 0.689605051279068 \n",
      "At 499 epoch, Training Loss: 0.6822564847767352 \n",
      "At 499 epoch, Validation Loss: 0.6897731214761733 \n",
      "At 500 epoch, Training Loss: 0.680737190321088 \n",
      "At 500 epoch, Validation Loss: 0.6903733164072039 \n"
     ]
    }
   ],
   "source": [
    "n_epoch=500\n",
    "model,train_loss,valid_loss=train(n_epoch,loaders,model,optimizer,criterion,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-1.4138e-01, -6.0462e-02, -9.5733e-02,  7.3870e-02, -7.3791e-03,\n",
       "                        2.1429e-02, -1.9180e-01, -1.7724e-01, -2.0179e-01, -1.4766e-01,\n",
       "                       -3.3372e-02,  5.3046e-02,  9.7516e-02, -1.6881e-01, -2.3481e-01,\n",
       "                       -1.8919e-01,  1.1018e-01, -7.7994e-03,  1.5687e-01, -2.5337e-01,\n",
       "                        1.1768e-01, -8.4001e-02,  4.9440e-02],\n",
       "                      [ 3.1972e-02,  1.6994e-01,  2.1667e-01,  1.5361e-01, -7.3494e-02,\n",
       "                        2.3099e-01, -5.7540e-02,  1.1945e-01,  4.1076e-02,  1.5322e-01,\n",
       "                       -5.9745e-02, -3.5819e-03,  1.9669e-02, -3.3753e-02,  1.0979e-01,\n",
       "                       -5.8455e-02,  1.5564e-01,  5.6444e-02, -1.9274e-01,  1.2531e-01,\n",
       "                       -5.8726e-02,  3.0291e-02,  2.2326e-01],\n",
       "                      [-3.4549e-01,  5.4459e-03, -2.0313e-03, -7.6889e-02,  9.8695e-02,\n",
       "                       -5.8035e-02,  1.2416e-01, -1.4356e-01,  7.0212e-03, -1.9356e-01,\n",
       "                       -8.0920e-02,  1.7387e-01, -1.1834e-01,  4.5555e-02, -9.0820e-02,\n",
       "                        3.8345e-02,  8.4199e-02,  1.7730e-01, -1.0502e-01,  5.2861e-02,\n",
       "                        6.8015e-02, -1.2005e-02, -1.8368e-01],\n",
       "                      [-1.5712e-01, -1.2655e-01,  1.4712e-01,  1.2944e-02, -1.0475e-01,\n",
       "                       -2.2820e-01, -2.5047e-01,  3.1982e-01, -8.8402e-02, -7.1620e-02,\n",
       "                       -5.5501e-03,  5.3586e-04, -6.8134e-02, -8.7400e-02, -5.0809e-02,\n",
       "                       -1.6047e-01, -2.5002e-02,  2.6926e-01, -3.4168e-01, -1.1626e-01,\n",
       "                       -3.3152e-03,  6.0534e-02,  3.4858e-02],\n",
       "                      [-3.1577e-01,  5.4259e-02, -5.0264e-02, -4.5419e-01, -5.6248e-02,\n",
       "                        1.3978e-01,  6.6269e-02,  7.4188e-02,  6.4779e-02, -6.2630e-02,\n",
       "                       -1.1931e-03, -3.4724e-02, -8.0519e-02,  1.7925e-01, -2.4711e-02,\n",
       "                        1.0312e-01,  3.1738e-02,  1.6980e-01, -1.5291e-01, -5.8899e-02,\n",
       "                        3.7028e-02, -1.2966e-03, -2.3603e-01],\n",
       "                      [-1.9748e-01,  3.3943e-03,  2.2657e-01, -1.8647e-01,  7.6266e-02,\n",
       "                        5.9221e-02, -6.0962e-02, -2.3389e-01, -1.0671e-01, -2.1749e-01,\n",
       "                        3.4592e-02, -2.1282e-02, -2.9883e-01,  1.9755e-02, -1.5799e-01,\n",
       "                       -1.7203e-01,  2.7630e-01,  1.9004e-01, -1.0708e-01,  9.3070e-02,\n",
       "                       -4.9937e-02,  5.7214e-02, -1.4531e-01],\n",
       "                      [-1.9164e-01, -7.2796e-02,  7.2079e-02,  1.0432e-01, -8.4801e-02,\n",
       "                       -4.6054e-02, -1.7516e-01,  3.4091e-01, -2.7431e-01,  2.2829e-01,\n",
       "                        8.4019e-02,  1.7979e-02,  6.6847e-02,  7.0687e-02, -2.5427e-01,\n",
       "                       -2.1881e-02, -1.2095e-01,  4.7704e-02,  5.2911e-02, -1.1463e-01,\n",
       "                        9.5224e-02, -4.8272e-02, -2.4108e-03],\n",
       "                      [-3.9025e-01,  8.2687e-02,  5.7023e-02, -2.3712e-01, -3.3192e-03,\n",
       "                        3.5155e-02, -1.5539e-01, -6.6922e-02, -1.0673e-01, -6.7642e-02,\n",
       "                       -1.8067e-02,  2.7525e-02, -3.9473e-01,  1.5101e-02,  1.1868e-01,\n",
       "                        4.4894e-02, -9.4287e-02,  1.1495e-01,  2.4936e-03, -2.4074e-01,\n",
       "                       -4.9021e-02,  7.0296e-02,  4.6566e-02],\n",
       "                      [ 6.8814e-02, -3.8309e-02, -2.3933e-02, -6.6357e-02,  2.9342e-01,\n",
       "                       -4.4309e-02, -1.3452e-02, -2.2491e-02, -1.0630e-01,  1.6195e-01,\n",
       "                        4.4882e-02,  4.0316e-02, -8.6772e-02,  6.5664e-02,  4.1480e-01,\n",
       "                       -6.4618e-02, -1.8455e-01,  2.1162e-02,  3.3675e-03,  1.6356e-01,\n",
       "                        1.1472e-01, -7.3022e-03,  5.8143e-02],\n",
       "                      [-1.6332e-01,  8.6014e-02, -3.4085e-02, -2.4024e-01, -5.2176e-02,\n",
       "                        6.3273e-02, -8.1689e-02, -2.4273e-01, -1.3460e-02, -9.0814e-02,\n",
       "                        6.5192e-03, -8.9727e-03, -2.3804e-01,  6.0833e-02,  2.3371e-01,\n",
       "                        5.0551e-02, -5.3515e-02,  1.6593e-02,  1.4495e-02, -5.0003e-01,\n",
       "                       -8.1890e-02,  1.9092e-01, -9.6755e-02],\n",
       "                      [-3.8436e-01, -3.7816e-02,  1.2583e-01,  3.3043e-02, -8.5492e-02,\n",
       "                       -7.7574e-02, -2.2490e-01,  1.8329e-01,  1.3252e-01, -6.9279e-02,\n",
       "                       -7.0679e-02, -7.2863e-02, -6.4524e-02, -1.3985e-02, -1.3220e-01,\n",
       "                       -4.9413e-02,  3.1855e-02,  1.5742e-01, -1.9392e-01,  2.8450e-02,\n",
       "                        1.2411e-02,  1.3617e-03,  9.4449e-02],\n",
       "                      [-1.8982e-01, -8.0528e-02, -1.1795e-01,  1.9112e-01,  8.1681e-03,\n",
       "                        4.9197e-02,  9.2709e-02,  1.3697e-01, -3.1167e-02, -2.7501e-01,\n",
       "                        1.8398e-02,  5.3360e-02, -1.5738e-01, -9.9894e-02, -2.2532e-01,\n",
       "                       -7.0041e-02,  2.6971e-01,  2.8246e-01, -1.2463e-01,  2.5390e-02,\n",
       "                       -3.3831e-02,  1.1854e-02,  3.5196e-02],\n",
       "                      [ 2.0062e-02, -3.8989e-02, -4.2499e-02, -1.5915e-01,  8.3461e-03,\n",
       "                       -2.2140e-01, -1.3069e-01, -1.8353e-01,  5.5583e-02,  4.8589e-02,\n",
       "                       -6.3571e-02,  1.2748e-02, -3.2445e-02,  2.7622e-01,  8.3253e-02,\n",
       "                       -8.6860e-02, -3.2854e-02, -1.1822e-02,  1.7949e-01,  2.8058e-01,\n",
       "                       -1.5568e-01, -3.5716e-04, -3.3540e-01],\n",
       "                      [ 1.2811e-01, -1.6090e-02, -9.1900e-02, -1.9137e-03, -7.1852e-02,\n",
       "                        1.0308e-01,  3.0133e-02,  1.2748e-01,  1.0426e-01,  5.9105e-02,\n",
       "                       -5.2017e-02, -4.0115e-02, -5.2361e-02,  2.0622e-01,  5.0749e-03,\n",
       "                        1.4101e-01,  2.2397e-01, -5.7292e-02,  3.3848e-01,  1.2583e-01,\n",
       "                        1.0204e-01, -6.2865e-02,  1.5455e-01],\n",
       "                      [-9.2901e-02, -2.1070e-02, -5.2775e-02,  2.4890e-01, -1.2441e-01,\n",
       "                        2.2483e-01, -1.0117e-01, -4.8978e-01,  3.7259e-02,  2.4598e-02,\n",
       "                       -5.7746e-02,  4.7084e-02,  2.1289e-01, -1.5885e-02, -1.1534e-01,\n",
       "                        3.1867e-01, -2.3543e-01, -1.0702e-01,  9.8497e-02,  9.3173e-02,\n",
       "                       -5.2557e-02,  1.2615e-01, -1.4509e-01],\n",
       "                      [-2.3144e-01,  7.1423e-02, -7.2560e-02, -6.2771e-03, -8.9812e-02,\n",
       "                       -6.2836e-02, -8.4812e-02, -5.7088e-02,  1.2249e-02,  2.2920e-02,\n",
       "                        3.3725e-02, -3.0007e-02, -9.8170e-02, -1.1248e-01, -6.4279e-02,\n",
       "                        4.1100e-02, -1.3241e-01,  1.2557e-02,  2.3090e-01, -3.7016e-01,\n",
       "                       -1.5062e-02, -2.8340e-02,  1.4893e-01],\n",
       "                      [ 1.8644e-03, -7.5402e-02,  3.8253e-03,  1.6983e-01, -9.7511e-02,\n",
       "                       -2.2279e-02, -4.1267e-01,  1.4671e-01, -2.6005e-01, -1.4702e-01,\n",
       "                        1.2109e-02,  1.4922e-01,  5.2895e-02, -8.8403e-02, -2.7237e-02,\n",
       "                       -1.3498e-01, -5.0932e-02,  5.9557e-02, -1.1902e-01, -2.5342e-01,\n",
       "                       -2.5744e-02,  2.7471e-02,  8.3935e-02],\n",
       "                      [-2.4825e-03,  3.0464e-02, -6.5241e-02, -2.7469e-02,  2.3374e-01,\n",
       "                       -4.5205e-02,  7.4747e-03,  2.9738e-01, -1.3573e-01,  2.7808e-02,\n",
       "                        1.0780e-01,  5.1962e-02, -5.7914e-02, -1.2059e-01,  5.3699e-01,\n",
       "                       -9.6424e-02, -8.4113e-02,  8.0555e-02, -9.4696e-02,  1.9671e-01,\n",
       "                        7.9707e-02,  5.2024e-03,  5.0578e-02],\n",
       "                      [-1.7003e-01,  9.3979e-02,  1.2825e-01, -1.1474e-01,  4.2503e-02,\n",
       "                        3.8009e-01,  6.5455e-02,  3.7488e-01, -4.3406e-02,  2.5241e-01,\n",
       "                       -1.2078e-02, -5.9547e-02,  1.4915e-01,  6.8237e-02, -1.5936e-02,\n",
       "                       -1.5182e-02,  1.8120e-01, -5.1667e-02, -1.4328e-01,  1.9648e-01,\n",
       "                       -9.0105e-03, -5.0236e-02,  2.3206e-01],\n",
       "                      [-1.2004e-01, -9.7659e-02,  2.2838e-02,  1.7236e-01, -9.9424e-02,\n",
       "                        3.4028e-01, -4.8044e-03, -8.9548e-02, -5.3367e-02,  1.9530e-01,\n",
       "                       -2.1422e-02, -1.2644e-01,  2.4192e-01,  6.3030e-02,  1.0777e-01,\n",
       "                        2.6097e-01, -8.5357e-02, -3.4937e-02,  2.6424e-01,  1.5263e-01,\n",
       "                        3.6803e-02, -9.4556e-03,  1.8174e-01],\n",
       "                      [ 1.5913e-02, -2.6102e-02, -1.4756e-02, -3.7908e-01, -1.2970e-02,\n",
       "                        2.1946e-01,  3.8313e-02, -1.0441e-01, -1.9278e-02, -1.4465e-01,\n",
       "                       -4.7264e-02,  3.6500e-02, -3.6088e-01,  6.5861e-02, -5.9293e-02,\n",
       "                        1.1036e-01,  4.5424e-02,  1.4494e-01, -1.4055e-01, -2.9355e-01,\n",
       "                        2.6063e-02, -4.8603e-02, -1.0559e-01],\n",
       "                      [-3.4975e-01, -4.4516e-02,  1.1523e-01, -9.6593e-02,  7.0929e-03,\n",
       "                        9.4311e-02,  8.9671e-02,  1.3532e-01,  1.0114e-01,  2.7363e-01,\n",
       "                        6.1278e-03, -9.1054e-02, -1.5450e-01,  4.5651e-02, -2.2087e-01,\n",
       "                       -2.1977e-02,  1.9747e-01,  1.3653e-01, -1.8105e-01, -2.3292e-01,\n",
       "                        1.0556e-01,  3.3131e-02, -1.8437e-01],\n",
       "                      [ 4.8289e-03, -6.7815e-02, -6.7187e-02,  5.3029e-02,  1.7631e-01,\n",
       "                        1.0327e-01,  2.3460e-02, -4.1790e-02,  8.0325e-02, -8.1212e-02,\n",
       "                        5.9018e-02, -1.8407e-03, -2.9148e-02,  1.7956e-01,  2.8945e-02,\n",
       "                        1.7387e-01, -1.3346e-01,  2.5457e-02, -6.5633e-02,  2.7884e-01,\n",
       "                        1.0465e-01, -7.2401e-02, -3.7341e-02],\n",
       "                      [-7.0403e-02,  2.1855e-02,  2.9331e-03,  1.4791e-01,  1.9184e-01,\n",
       "                       -1.4907e-01,  1.2917e-01, -4.2349e-01,  6.8768e-03,  5.6502e-02,\n",
       "                       -4.7431e-03, -1.2433e-01, -5.8366e-02, -7.9280e-02, -2.9671e-01,\n",
       "                       -1.2143e-01,  1.2278e-01,  1.5626e-02,  1.8026e-01, -2.9185e-01,\n",
       "                        1.5118e-01, -2.4824e-02,  1.2895e-01],\n",
       "                      [-1.0419e-01, -1.0239e-02,  3.7371e-03,  2.0356e-01, -1.9985e-01,\n",
       "                        4.3553e-02, -2.3917e-01,  6.1035e-02, -3.0223e-01,  8.9534e-02,\n",
       "                        8.9621e-02,  4.1860e-02,  7.4200e-02, -6.6515e-02, -1.8584e-02,\n",
       "                       -1.3248e-01,  1.2466e-01,  6.2166e-02,  2.4857e-01, -3.4003e-02,\n",
       "                       -6.6083e-02, -3.3209e-02,  9.8868e-02],\n",
       "                      [ 1.7787e-01,  1.5843e-02, -3.8209e-02,  1.6471e-02, -5.0645e-02,\n",
       "                       -8.0922e-02,  3.4950e-01,  2.6146e-01,  1.8740e-01,  1.0783e-01,\n",
       "                       -2.0675e-02,  1.7094e-01, -2.9460e-01,  8.8601e-02,  2.1978e-01,\n",
       "                       -1.3176e-01,  7.5746e-02, -2.7248e-01, -2.0661e-01,  9.2329e-02,\n",
       "                       -6.2918e-03, -2.5460e-02, -1.4203e-01],\n",
       "                      [ 9.1303e-02, -8.4928e-03,  1.6780e-02, -4.7508e-03,  1.0240e-01,\n",
       "                       -1.5492e-01, -1.0900e-02, -3.1239e-02,  1.3291e-02, -1.2660e-01,\n",
       "                        1.4863e-02,  1.8888e-01,  4.3584e-02,  9.0288e-02,  1.8386e-01,\n",
       "                        1.6085e-01, -1.7942e-01,  2.7624e-02,  2.8422e-01,  3.1702e-01,\n",
       "                        8.6023e-02, -1.0787e-01, -1.8347e-01],\n",
       "                      [ 3.6640e-02, -8.4896e-02, -5.9982e-02,  1.7596e-01,  6.2411e-02,\n",
       "                        2.1965e-01, -2.2135e-01, -2.7542e-02, -2.1787e-01, -2.3827e-01,\n",
       "                       -2.6505e-02,  4.9458e-02, -3.6769e-01, -2.7326e-02, -2.7760e-01,\n",
       "                       -1.3490e-01,  1.8665e-01,  1.5142e-01, -2.8105e-02, -2.9940e-01,\n",
       "                       -1.0383e-01,  8.5440e-02,  3.2395e-02],\n",
       "                      [ 8.8899e-02, -4.9680e-02,  4.5870e-02, -1.5437e-01, -6.1128e-02,\n",
       "                       -1.5689e-01,  2.1006e-01, -1.8294e-01,  6.4499e-02, -1.1180e-01,\n",
       "                        5.6542e-02,  1.7908e-01, -4.6691e-01, -5.7724e-02,  3.5431e-01,\n",
       "                       -1.7815e-01,  1.5527e-02, -1.8416e-01,  6.9775e-02,  1.5498e-01,\n",
       "                        1.1997e-01,  4.5977e-02, -1.3211e-01],\n",
       "                      [ 4.2258e-02, -1.2498e-01, -6.5891e-02,  3.4852e-01, -1.9607e-02,\n",
       "                        1.4863e-01, -2.1137e-01,  1.3940e-01, -1.7799e-01, -2.5749e-01,\n",
       "                        9.9055e-04,  6.3505e-02, -5.4447e-02, -2.3262e-01, -2.7884e-01,\n",
       "                       -1.0358e-01,  2.0161e-01,  2.8882e-01,  2.2795e-02, -2.1592e-01,\n",
       "                       -4.3336e-02,  3.6721e-03,  1.3778e-02],\n",
       "                      [ 1.3056e-01, -2.3976e-02,  6.2644e-02,  2.0397e-02,  3.6446e-02,\n",
       "                       -1.9624e-01, -1.6164e-01,  4.6490e-01, -4.7270e-03, -4.5758e-02,\n",
       "                        9.9103e-03,  2.1016e-02,  1.5456e-01,  5.9246e-02, -2.9320e-01,\n",
       "                       -6.7073e-03,  1.4483e-02,  3.6486e-01,  8.2530e-06,  2.7163e-01,\n",
       "                       -3.2829e-02,  7.0164e-02, -1.7269e-02],\n",
       "                      [-3.0902e-02, -3.9198e-02,  4.1539e-02,  2.2167e-02,  1.6177e-01,\n",
       "                        1.4138e-01, -1.1430e-01, -3.1882e-01, -1.6945e-01, -2.0430e-01,\n",
       "                       -4.5060e-02,  1.6957e-02, -3.7718e-01,  6.4378e-02,  9.3210e-03,\n",
       "                        8.5906e-02,  1.0320e-01, -1.4434e-01, -5.9837e-02, -3.1234e-01,\n",
       "                       -3.6830e-02, -8.0752e-02,  5.5586e-02]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 6.6868e-04, -3.0423e-04, -4.5200e-04,  4.6388e-04, -1.0369e-04,\n",
       "                       3.0120e-04, -1.4982e-04,  2.2949e-04,  9.2737e-06,  2.0970e-04,\n",
       "                       2.5912e-04, -5.2494e-05, -5.5362e-04, -4.3291e-04, -1.8604e-04,\n",
       "                      -4.7586e-04,  7.9744e-04, -8.2396e-04, -2.4008e-04,  8.2616e-04,\n",
       "                       1.3438e-04,  4.1515e-05, -2.9324e-04,  3.2670e-04, -1.2504e-04,\n",
       "                       1.1966e-04,  5.1033e-04, -3.5211e-04,  1.8624e-04, -6.9713e-05,\n",
       "                       5.1021e-04,  7.0874e-04])),\n",
       "             ('norm1.weight',\n",
       "              tensor([0.2975, 0.2358, 0.1645, 0.2556, 0.2554, 0.2073, 0.2430, 0.2261, 0.1782,\n",
       "                      0.2165, 0.2395, 0.2454, 0.2215, 0.1651, 0.2517, 0.1391, 0.2241, 0.2251,\n",
       "                      0.2564, 0.2675, 0.1945, 0.2566, 0.1729, 0.2618, 0.2351, 0.2353, 0.1470,\n",
       "                      0.3339, 0.2378, 0.3478, 0.2223, 0.2323])),\n",
       "             ('norm1.bias',\n",
       "              tensor([-0.4620, -0.2330, -0.1624, -0.1887, -0.3401, -0.2159, -0.3597, -0.1825,\n",
       "                      -0.1453, -0.2092, -0.2169, -0.3271, -0.3099, -0.1571, -0.2432, -0.1284,\n",
       "                      -0.2748, -0.2261, -0.3799, -0.4367, -0.1556, -0.3313, -0.3132, -0.3358,\n",
       "                      -0.3423, -0.3926, -0.0841, -0.3517, -0.3587, -0.3934, -0.3931, -0.2055])),\n",
       "             ('norm1.running_mean',\n",
       "              tensor([-0.6593,  0.6511, -0.2696, -0.5206, -0.3013, -0.4581, -0.1113, -0.6022,\n",
       "                       0.3926, -0.5792, -0.3376, -0.1119, -0.2621,  0.7059, -0.0866, -0.4406,\n",
       "                      -0.5030,  0.4849,  0.7391,  0.6525, -0.5289, -0.1220,  0.3463, -0.2448,\n",
       "                      -0.0323,  0.2477,  0.4102, -0.5565, -0.1693, -0.2551,  0.4484, -0.6185])),\n",
       "             ('norm1.running_var',\n",
       "              tensor([0.0007, 0.0011, 0.0004, 0.0005, 0.0007, 0.0004, 0.0008, 0.0004, 0.0005,\n",
       "                      0.0004, 0.0006, 0.0005, 0.0007, 0.0006, 0.0005, 0.0004, 0.0005, 0.0006,\n",
       "                      0.0005, 0.0005, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0004, 0.0004,\n",
       "                      0.0005, 0.0004, 0.0005, 0.0004, 0.0005])),\n",
       "             ('norm1.num_batches_tracked', tensor(53440)),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-1.1046e-01, -3.5672e-02,  1.0341e-01,  1.6865e-01,  7.9291e-02,\n",
       "                        1.3026e-01, -7.8960e-02,  8.2289e-02, -3.5050e-02,  5.0537e-02,\n",
       "                        1.0196e-01, -4.9339e-02, -3.6496e-02, -1.3323e-01,  3.9603e-02,\n",
       "                        9.8802e-02, -7.9019e-03, -4.5665e-02, -5.7997e-02, -1.2227e-02,\n",
       "                        1.3634e-01,  4.7867e-02, -7.3159e-02,  4.5364e-02,  1.1244e-01,\n",
       "                        2.3621e-04, -8.5777e-02, -1.3303e-01, -9.2756e-02, -1.3441e-01,\n",
       "                        1.7904e-01,  1.6428e-02],\n",
       "                      [-7.9183e-02,  4.8688e-02, -7.7597e-02, -8.7539e-03, -9.3832e-02,\n",
       "                       -5.9063e-02, -5.6348e-02, -4.4516e-02,  5.8846e-02,  1.1609e-03,\n",
       "                       -1.8432e-01, -6.4017e-02,  1.1496e-01,  3.4926e-02,  3.1403e-02,\n",
       "                        1.4996e-03, -1.1404e-01,  1.0478e-02,  1.7459e-01,  1.6837e-01,\n",
       "                       -1.5834e-01, -7.2099e-02, -7.8613e-02, -4.5224e-02,  4.3756e-03,\n",
       "                        1.5804e-01, -5.8303e-03, -6.8347e-02,  6.7907e-02, -6.1086e-02,\n",
       "                       -1.2304e-01, -1.3093e-01],\n",
       "                      [ 7.5830e-02, -9.7141e-02,  7.7873e-03,  4.7215e-02,  1.5203e-01,\n",
       "                        7.2162e-02,  1.3325e-01,  6.6795e-02, -2.1225e-02,  7.6214e-02,\n",
       "                        5.1683e-02,  3.9185e-02, -9.2914e-02, -7.6789e-02, -8.7917e-02,\n",
       "                        8.0793e-02,  1.2734e-01, -1.1684e-01,  2.0459e-02, -1.5798e-01,\n",
       "                        2.9890e-02,  1.3797e-01,  6.5342e-02,  1.7924e-01,  1.0957e-01,\n",
       "                       -8.9718e-02, -1.0602e-01,  1.6261e-01, -1.5563e-01,  8.0065e-02,\n",
       "                        1.7241e-01,  2.5516e-02],\n",
       "                      [ 1.3225e-01, -1.2865e-01,  5.0931e-02,  7.1963e-03, -5.0592e-02,\n",
       "                        9.9638e-02,  1.4723e-01,  4.6789e-02, -8.5762e-02,  6.3458e-02,\n",
       "                       -1.8855e-01,  1.7947e-01, -6.7878e-02, -5.0626e-02, -1.5006e-01,\n",
       "                        6.5859e-02,  5.8530e-02, -5.3886e-03, -2.0704e-02, -1.4343e-01,\n",
       "                       -2.0213e-02, -4.3631e-02,  7.2985e-02,  1.7541e-01,  9.6549e-02,\n",
       "                       -7.9572e-02, -7.1227e-02,  8.2602e-02, -8.6611e-02,  1.4186e-01,\n",
       "                        6.8686e-03,  6.1802e-02],\n",
       "                      [ 8.4544e-02, -1.1340e-01,  5.4800e-02,  4.9439e-02,  7.3101e-02,\n",
       "                        6.4333e-02, -1.8017e-02,  6.8989e-02, -7.1098e-02,  6.6649e-02,\n",
       "                        5.2815e-02,  7.1694e-02, -5.9413e-02, -6.5583e-02, -1.4392e-01,\n",
       "                        7.7916e-02, -1.2181e-02, -1.4616e-01, -1.1138e-02, -1.1503e-01,\n",
       "                        1.2313e-01,  1.3841e-01,  1.1590e-01,  1.1005e-01,  7.1682e-02,\n",
       "                       -9.0736e-02, -1.4806e-01,  8.1017e-02, -1.2622e-01,  5.7766e-02,\n",
       "                        2.1805e-01,  6.6757e-02],\n",
       "                      [ 1.8243e-01, -6.1487e-02,  3.2152e-03,  5.4812e-02,  1.7514e-01,\n",
       "                        8.6115e-02,  1.1560e-01,  2.6819e-02, -1.4603e-01,  8.6798e-02,\n",
       "                       -1.1742e-01,  1.9976e-01, -1.2086e-01, -1.7807e-02, -7.3217e-02,\n",
       "                        3.5450e-02,  7.5567e-02, -1.7495e-01,  2.3299e-02, -6.9111e-02,\n",
       "                        3.0793e-02,  1.5791e-01,  3.8300e-02, -1.3114e-02,  1.2014e-01,\n",
       "                       -1.0233e-01, -5.9235e-02,  4.7440e-02, -1.2175e-01,  1.2936e-01,\n",
       "                        7.5745e-02,  1.0836e-01],\n",
       "                      [-1.0013e-01, -1.5940e-01,  8.7010e-02,  1.2883e-01,  1.1005e-01,\n",
       "                        9.8210e-02,  7.1987e-02,  5.2722e-02, -9.2349e-02,  1.3548e-01,\n",
       "                        9.7460e-02, -1.6043e-02, -4.8738e-02, -8.0501e-02, -7.0997e-02,\n",
       "                        1.6423e-02,  2.1838e-02, -1.0271e-01, -1.1662e-01, -1.3163e-01,\n",
       "                        7.8454e-02, -5.6515e-02,  1.6350e-02,  9.7372e-02,  1.0779e-01,\n",
       "                       -2.3883e-02, -4.4542e-02, -1.1220e-01, -1.1403e-01, -4.3194e-02,\n",
       "                       -8.1955e-03,  8.1641e-02],\n",
       "                      [ 1.4780e-01, -4.2907e-02,  4.8149e-02,  8.7916e-02,  1.9764e-01,\n",
       "                       -5.1048e-02,  1.8179e-01,  8.1030e-02, -5.2016e-02,  1.4439e-01,\n",
       "                       -1.2059e-01,  1.5503e-01, -5.9122e-02, -4.9395e-02, -1.9851e-01,\n",
       "                        7.2556e-02,  5.4244e-02, -4.5682e-02, -4.5460e-03, -1.2173e-01,\n",
       "                        5.1153e-02,  8.1036e-02,  1.2221e-01,  1.3545e-01,  4.8951e-02,\n",
       "                       -3.7165e-02,  1.0977e-03,  1.2783e-01, -7.8870e-02,  1.1024e-01,\n",
       "                        8.3359e-02,  8.9821e-02],\n",
       "                      [-7.9992e-02, -3.9905e-03, -1.2695e-01, -8.9718e-02, -8.4422e-02,\n",
       "                       -3.9842e-02, -8.9907e-02, -9.9701e-02,  6.4279e-02, -6.7762e-02,\n",
       "                       -8.4611e-02, -3.2459e-02,  1.0924e-01,  6.7897e-02,  2.8895e-02,\n",
       "                       -2.8615e-02, -1.7546e-01,  2.7394e-02,  1.5750e-01,  1.8522e-01,\n",
       "                       -1.2732e-01, -3.4332e-02, -1.5741e-01, -3.5361e-02,  1.5582e-02,\n",
       "                        1.3957e-01,  5.5677e-02, -4.1326e-02,  9.6478e-02, -6.7983e-02,\n",
       "                       -1.7484e-01, -7.4466e-02],\n",
       "                      [ 1.1659e-01, -7.0486e-02, -1.2367e-02,  5.2458e-02, -2.7009e-02,\n",
       "                        1.0437e-01,  1.6954e-01,  7.4400e-02, -7.7306e-02,  5.0097e-02,\n",
       "                       -3.9890e-02,  1.6033e-01, -1.3584e-01, -6.5307e-02, -1.0432e-01,\n",
       "                        5.4297e-02,  8.0678e-02, -1.1121e-01, -3.0312e-02, -1.3745e-01,\n",
       "                        4.7149e-02,  1.3743e-01, -1.0559e-02,  1.0240e-01,  1.0433e-01,\n",
       "                       -8.4431e-02, -6.9861e-02,  9.1448e-02, -1.1575e-01,  9.6942e-02,\n",
       "                        1.4045e-01,  3.3428e-02],\n",
       "                      [-6.5083e-03, -7.2375e-02, -3.3189e-02,  1.1870e-01, -4.6136e-03,\n",
       "                        1.3741e-01,  8.3985e-02,  9.7934e-02, -1.2012e-01,  9.5090e-02,\n",
       "                        3.9372e-02,  1.2113e-01, -7.2577e-02, -6.3081e-02, -6.8181e-02,\n",
       "                        4.0739e-02,  5.9842e-02, -1.0678e-01, -3.3129e-02, -1.4483e-01,\n",
       "                        6.6687e-03,  1.6959e-01,  1.3430e-01,  9.4289e-02,  1.1552e-01,\n",
       "                       -8.9555e-02, -5.1856e-02,  8.0925e-02, -1.4630e-01,  1.1246e-01,\n",
       "                        1.4255e-01,  7.7004e-02],\n",
       "                      [-1.3481e-01, -1.0839e-02, -6.8252e-02, -6.3725e-02, -1.0880e-01,\n",
       "                        1.3365e-02, -1.1370e-01, -5.4762e-02,  1.8786e-02, -4.1877e-02,\n",
       "                       -1.0089e-01, -9.7174e-03,  1.1705e-01,  9.2135e-02,  3.4993e-02,\n",
       "                       -6.7870e-02, -1.3863e-01,  1.1538e-02,  1.3107e-01,  1.9634e-01,\n",
       "                       -8.5704e-02, -1.5336e-02, -7.9384e-02, -3.2106e-02, -4.0889e-02,\n",
       "                        1.6420e-01, -1.7425e-02, -3.7005e-02,  9.9173e-02,  5.1796e-03,\n",
       "                       -1.7890e-01, -1.3683e-01],\n",
       "                      [ 1.4568e-01,  2.4070e-03, -1.5360e-02,  9.8699e-04, -3.9213e-02,\n",
       "                       -2.1682e-01,  2.3952e-01,  5.2869e-02,  1.3836e-02,  4.1250e-03,\n",
       "                       -1.5178e-01,  1.7904e-01, -3.0965e-02,  1.4541e-02, -1.2214e-01,\n",
       "                       -4.4000e-02,  8.4496e-02, -2.1816e-02, -2.3602e-03, -1.3177e-01,\n",
       "                        1.3196e-02, -5.7608e-02,  1.5064e-01,  3.4182e-03,  6.2657e-02,\n",
       "                       -8.3612e-03,  8.0847e-02,  1.7057e-01, -1.6603e-02, -2.4840e-02,\n",
       "                        1.3637e-01,  1.1635e-01],\n",
       "                      [ 7.6022e-02, -6.6847e-02,  3.0215e-02,  1.0679e-01,  1.5953e-01,\n",
       "                        1.3631e-01,  1.8974e-01,  5.9848e-02, -7.7325e-02,  1.0958e-01,\n",
       "                       -7.4834e-04,  1.6654e-01, -1.8917e-03, -7.5520e-02, -1.1625e-01,\n",
       "                        5.3015e-02,  4.1199e-02, -1.1989e-01, -1.0830e-02, -1.5802e-01,\n",
       "                        4.7686e-02,  1.3651e-01,  1.5439e-01,  1.0743e-01,  1.1051e-01,\n",
       "                       -9.8059e-02, -9.0543e-02,  6.7015e-02, -1.0824e-01,  5.4020e-02,\n",
       "                        1.0851e-02,  4.1741e-02],\n",
       "                      [-9.9109e-02,  1.5202e-02, -9.3158e-02,  3.2186e-02, -8.7673e-02,\n",
       "                       -2.7854e-02, -6.8498e-02, -4.2526e-02,  2.3094e-02, -6.8829e-02,\n",
       "                       -1.9201e-01, -3.1342e-02,  8.9232e-02,  5.0699e-02,  6.9299e-02,\n",
       "                       -1.8143e-03, -3.9833e-02,  6.7142e-02,  1.4189e-01,  1.9787e-01,\n",
       "                       -3.4237e-02, -9.7989e-04, -1.1638e-01, -7.2656e-02, -3.5400e-02,\n",
       "                        1.7155e-01,  2.9809e-02, -8.4778e-02,  9.3325e-02, -4.5572e-02,\n",
       "                       -1.4494e-01, -1.2350e-01],\n",
       "                      [-4.7499e-02,  5.7580e-02, -1.0244e-01, -1.0434e-01, -1.2066e-01,\n",
       "                       -7.2846e-02, -1.1511e-01, -6.9975e-02,  2.7636e-02, -8.8763e-02,\n",
       "                       -1.2952e-01, -4.3742e-02,  5.7228e-02,  1.5117e-02,  7.2095e-02,\n",
       "                        7.5802e-03, -1.1533e-01,  6.5113e-02,  1.0560e-01,  1.6863e-01,\n",
       "                       -4.6922e-02, -3.5844e-02, -1.2576e-01, -7.8089e-02, -6.3993e-02,\n",
       "                        1.4926e-01,  4.2564e-02, -4.0188e-02,  8.1603e-02, -9.2139e-02,\n",
       "                       -1.7815e-01, -5.3772e-02]])),\n",
       "             ('norm2.weight',\n",
       "              tensor([0.2654, 0.2634, 0.3086, 0.3093, 0.2528, 0.3215, 0.2511, 0.2770, 0.2464,\n",
       "                      0.2675, 0.2282, 0.2434, 0.2781, 0.3140, 0.2769, 0.2934])),\n",
       "             ('norm2.bias',\n",
       "              tensor([ 0.0021, -0.0743,  0.0656,  0.0308,  0.0653,  0.0485,  0.0663,  0.0604,\n",
       "                      -0.0838,  0.0562,  0.0923, -0.0996, -0.0506,  0.0838, -0.0616, -0.0903])),\n",
       "             ('norm2.running_mean',\n",
       "              tensor([ 0.0075, -0.0156,  0.0115,  0.0006,  0.0058,  0.0061,  0.0018,  0.0145,\n",
       "                      -0.0157,  0.0062,  0.0110, -0.0146,  0.0054,  0.0115, -0.0100, -0.0155])),\n",
       "             ('norm2.running_var',\n",
       "              tensor([0.0019, 0.0020, 0.0025, 0.0024, 0.0023, 0.0026, 0.0018, 0.0028, 0.0019,\n",
       "                      0.0020, 0.0023, 0.0015, 0.0018, 0.0022, 0.0015, 0.0017])),\n",
       "             ('norm2.num_batches_tracked', tensor(53440)),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-2.0701e-01,  4.1313e-02, -2.5659e-01, -2.1590e-01, -2.1084e-01,\n",
       "                       -2.0823e-01, -2.0230e-01, -2.2633e-01,  4.4228e-02, -1.7804e-01,\n",
       "                       -1.8640e-01,  7.4565e-02, -1.6712e-01, -2.4487e-01,  3.9710e-02,\n",
       "                        6.5967e-02],\n",
       "                      [ 5.4281e-39,  7.5885e-39, -9.7273e-40, -2.3136e-39,  5.0058e-39,\n",
       "                       -5.4405e-40,  7.4922e-39,  3.0404e-39,  1.1800e-39, -2.2046e-39,\n",
       "                        9.5109e-40,  7.3600e-39, -2.6809e-39, -6.2373e-39,  7.4430e-39,\n",
       "                        4.5519e-39],\n",
       "                      [-5.8518e-39,  1.8189e-39, -3.3391e-39,  3.1548e-39, -4.3514e-39,\n",
       "                       -4.2881e-39, -4.7607e-39, -6.5174e-40, -6.0186e-39, -4.8714e-39,\n",
       "                       -4.0174e-39,  6.0101e-39, -5.9264e-39, -5.0496e-39, -5.2516e-39,\n",
       "                       -4.3676e-40],\n",
       "                      [-1.3288e-01, -1.5642e-01, -2.6217e-03,  7.4844e-02,  1.5174e-02,\n",
       "                        9.8555e-02, -1.0482e-01,  3.3716e-02, -2.2582e-01,  6.4354e-02,\n",
       "                       -3.1713e-02, -2.3086e-01,  1.0881e-01,  2.7329e-02, -2.5797e-01,\n",
       "                       -2.4104e-01],\n",
       "                      [-1.8052e-40,  8.0150e-39, -3.1420e-41,  2.3380e-39,  1.2634e-39,\n",
       "                       -1.8548e-39, -1.2275e-39,  5.7231e-40,  4.5802e-39, -2.8204e-39,\n",
       "                       -4.0623e-39, -2.5426e-39,  3.3860e-39, -1.2623e-39, -7.7724e-40,\n",
       "                       -4.7642e-39],\n",
       "                      [ 2.6094e-39,  3.0396e-39, -3.2115e-39,  3.8960e-39,  2.3859e-39,\n",
       "                        2.5019e-39,  2.6242e-39, -4.7886e-39, -6.3541e-39, -9.0979e-40,\n",
       "                        2.2583e-39,  3.9592e-39, -2.3869e-39,  2.6431e-39, -2.5022e-39,\n",
       "                        2.9595e-39],\n",
       "                      [-5.0235e-02, -1.1585e-01,  4.1251e-02,  9.4866e-02,  2.2179e-02,\n",
       "                       -1.7372e-01, -2.6767e-02, -9.0620e-03, -3.2349e-02, -2.2995e-02,\n",
       "                        3.8705e-02,  4.5114e-02,  9.9036e-02, -3.4689e-02,  9.8977e-03,\n",
       "                        6.5127e-02],\n",
       "                      [ 8.3684e-02,  1.8103e-02,  9.2580e-02,  2.3086e-02,  4.3573e-02,\n",
       "                        4.1243e-02,  9.9915e-02,  1.9985e-02,  4.8351e-03,  4.7291e-02,\n",
       "                        8.5156e-02,  5.1837e-03,  3.7885e-03,  1.0068e-01, -8.7556e-04,\n",
       "                       -5.4680e-03]])),\n",
       "             ('norm3.weight',\n",
       "              tensor([ 6.1279e-01,  1.2204e-39,  4.7686e-39,  1.6422e-01,  4.1605e-39,\n",
       "                      -1.1550e-40,  6.2711e-02,  6.1764e-02])),\n",
       "             ('norm3.bias',\n",
       "              tensor([ 6.4392e-02,  7.1977e-39,  2.1991e-39,  1.9239e-01, -2.1605e-39,\n",
       "                      -4.6861e-39, -1.9981e-03, -4.6333e-02])),\n",
       "             ('norm3.running_mean',\n",
       "              tensor([-2.4887e-01,  1.8752e-39, -4.6345e-39, -3.2030e-02, -5.5164e-40,\n",
       "                       1.0100e-39, -9.6809e-03,  7.6888e-02])),\n",
       "             ('norm3.running_var',\n",
       "              tensor([2.0195e-01, 5.6052e-45, 5.6052e-45, 2.1783e-02, 5.6052e-45, 5.6052e-45,\n",
       "                      5.3304e-03, 1.6385e-02])),\n",
       "             ('norm3.num_batches_tracked', tensor(53440)),\n",
       "             ('fc4.weight',\n",
       "              tensor([[-2.1113e-01,  8.8382e-39, -4.8126e-39,  9.6075e-02, -8.3330e-40,\n",
       "                        3.0242e-39,  1.4039e-02, -8.5458e-02],\n",
       "                      [ 1.9283e-01,  5.4953e-39, -4.5486e-39, -8.9148e-02,  4.0225e-39,\n",
       "                        3.1058e-39,  5.8525e-02,  5.5815e-02],\n",
       "                      [-1.6929e-01, -2.4996e-39,  2.6558e-39,  1.5693e-01, -4.3482e-39,\n",
       "                        1.8253e-39,  3.3308e-02, -5.3743e-02],\n",
       "                      [ 1.7509e-01,  6.2384e-39,  5.5611e-39, -1.0844e-01, -4.5673e-39,\n",
       "                       -3.5854e-39,  2.3880e-02,  7.5524e-02],\n",
       "                      [-2.2266e-01,  2.8885e-40, -4.2634e-39,  7.6925e-02,  3.9395e-39,\n",
       "                        6.5123e-39,  8.3340e-02, -1.7678e-02]])),\n",
       "             ('norm4.weight',\n",
       "              tensor([0.3767, 0.5331, 0.3784, 0.5116, 0.3511])),\n",
       "             ('norm4.bias',\n",
       "              tensor([ 0.1377, -0.1230,  0.1380, -0.1222,  0.1207])),\n",
       "             ('norm4.running_mean',\n",
       "              tensor([-0.0336,  0.0319, -0.0093,  0.0229, -0.0388])),\n",
       "             ('norm4.running_var',\n",
       "              tensor([0.0020, 0.0017, 0.0019, 0.0016, 0.0022])),\n",
       "             ('norm4.num_batches_tracked', tensor(53440)),\n",
       "             ('fc5.weight',\n",
       "              tensor([[ 0.1714, -0.2472,  0.1874, -0.2570,  0.1565]])),\n",
       "             ('fc5.bias', tensor([-0.0208]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"ccFraud.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_epochs=np.linspace(1,n_epoch,n_epoch).astype(dtype=int)\n",
    "plt.plot(x_epochs,train_loss)\n",
    "plt.plot(x_epochs,valid_loss)\n",
    "plt.xlabel(\"N Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend([\"Training\",\"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated AUC: 0.5527062731184419\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=np.linspace(0,1,10)\n",
    "TPR=[]\n",
    "FPR=[]\n",
    "for n in k:\n",
    "    outputs=[]\n",
    "    targets=[]\n",
    "    for data,target in test_loader:\n",
    "        output=model(data.float())\n",
    "        output=torch.sigmoid(output.cpu())\n",
    "        output=list(np.where(output<n,0,1)[:])\n",
    "        target=list(target.cpu().numpy())\n",
    "        outputs+=output\n",
    "        targets+=target\n",
    "\n",
    "    outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
    "    targets_of_model=np.array(targets).astype(dtype=int)\n",
    "    \n",
    "    #detect True Negatives, False Positives, False Negatives, True Positives\n",
    "    TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
    "    FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
    "    FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
    "    TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
    "\n",
    "    TPR.append(TP/(TP+FN))\n",
    "    FPR.append(FP/(FP+TN))\n",
    "    \n",
    "#Plot True Positive Rate and False Positive Rate\n",
    "plt.plot(FPR,TPR)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "AUC=0.0;\n",
    "for i in range(len(TPR)-1):\n",
    "    AUC+=np.abs((TPR[i]+TPR[i+1])*(FPR[i+1]-FPR[i])/2)\n",
    "  \n",
    "  \n",
    "print(\"Estimated AUC: {}\".format(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 569\n",
      "True Negatives: 544\n",
      "False Positives: 431\n",
      "False Negatives: 450\n",
      "Model Precision: 0.569\n",
      "Model Recall: 0.5583905789990187\n",
      "F1-score: 0.5636453689945518\n"
     ]
    }
   ],
   "source": [
    "outputs=[]\n",
    "targets=[]\n",
    "for data,target in test_loader:\n",
    "    output=model(data.float())\n",
    "    output=torch.sigmoid(output.cpu())\n",
    "    output=list(np.where(output<0.5,0,1)[:])\n",
    "    target=list(target.cpu().numpy())\n",
    "    outputs+=output\n",
    "    targets+=target\n",
    "\n",
    "outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
    "targets_of_model=np.array(targets).astype(dtype=int)\n",
    "\n",
    "\n",
    "TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
    "FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
    "FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
    "TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
    "\n",
    "\n",
    "print(\"True Positives: {}\".format(TP))\n",
    "print(\"True Negatives: {}\".format(TN))\n",
    "print(\"False Positives: {}\".format(FP))\n",
    "print(\"False Negatives: {}\".format(FN))\n",
    "\n",
    "\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "print(\"Model Precision: {}\".format(precision))\n",
    "print(\"Model Recall: {}\".format(recall))\n",
    "\n",
    "F1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"F1-score: {}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5581745235707122"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TP+TN)/(TP+FP+TN+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
